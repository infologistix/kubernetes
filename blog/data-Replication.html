<!doctype html>
<html lang="en" dir="ltr" class="blog-wrapper blog-post-page plugin-blog plugin-id-default">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.4.0">
<title data-rh="true">Data Replication with Confluent Cloud | KubeSpectra</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://kubespectra.com/blog/data-Replication"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docusaurus_tag" content="default"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docsearch:docusaurus_tag" content="default"><meta data-rh="true" property="og:title" content="Data Replication with Confluent Cloud | KubeSpectra"><meta data-rh="true" name="description" content="Apache Kafka as a Service by Confluent"><meta data-rh="true" property="og:description" content="Apache Kafka as a Service by Confluent"><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2022-03-04T00:00:00.000Z"><meta data-rh="true" property="article:author" content="https://github.com/HPG84,https://github.com/Chainsaw7"><meta data-rh="true" property="article:tag" content="Confluent,Kafka,Azure"><link data-rh="true" rel="icon" href="/img/kubespectra_logo.png"><link data-rh="true" rel="canonical" href="https://kubespectra.com/blog/data-Replication"><link data-rh="true" rel="alternate" href="https://kubespectra.com/blog/data-Replication" hreflang="en"><link data-rh="true" rel="alternate" href="https://kubespectra.com/blog/data-Replication" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="KubeSpectra RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="KubeSpectra Atom Feed"><link rel="stylesheet" href="/assets/css/styles.e061618a.css">
<link rel="preload" href="/assets/js/runtime~main.565f4334.js" as="script">
<link rel="preload" href="/assets/js/main.1ec76ff8.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}return t}()||function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><b class="navbar__title text--truncate">Home</b></a><a class="navbar__item navbar__link" href="/docs/projects">Projects</a><a class="navbar__item navbar__link" href="/docs/service">Service</a><a class="navbar__item navbar__link" href="/docs/events">Events</a><a class="navbar__item navbar__link" href="/docs/team">Team</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/blog">Blog</a></div><div class="navbar__items navbar__items--right"><div class="searchBox_ZlJk"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_pO2u margin-bottom--md">Recent posts</div><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/testdata">Test data sampling</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/lecture">First Lecture of Cloud Native Computing</a></li><li class="sidebarItem__DBe"><a aria-current="page" class="sidebarItemLink_mo7H sidebarItemLinkActive_I1ZP" href="/blog/data-Replication">Data Replication with Confluent Cloud</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/docker-in-wsl">Docker in WSL</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/webcrawler">Use Case for a WebCrawler</a></li></ul></nav></aside><main class="col col--7" itemscope="" itemtype="http://schema.org/Blog"><article itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h1 class="title_f1Hy" itemprop="headline">Data Replication with Confluent Cloud</h1><div class="container_mt6G margin-vert--md"><time datetime="2022-03-04T00:00:00.000Z" itemprop="datePublished">March 4, 2022</time> · <!-- -->9 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://github.com/HPG84" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://github.com/HPG84.png" alt="Harald P. Gerhards"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/HPG84" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Harald P. Gerhards</span></a></div><small class="avatar__subtitle" itemprop="description">Senior IT Consultant | Head of Cloud Engineering</small></div></div></div><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://github.com/Chainsaw7" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://github.com/Chainsaw7.png" alt="Emmanouil Goulidakis"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/Chainsaw7" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Emmanouil Goulidakis</span></a></div><small class="avatar__subtitle" itemprop="description">IT Consultant</small></div></div></div></div></header><div id="post-content" class="markdown" itemprop="articleBody"><p>Apache Kafka as a Service by Confluent</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="introduction">Introduction<a href="#introduction" class="hash-link" aria-label="Direct link to Introduction" title="Direct link to Introduction">​</a></h3><p>This article presents a proof of concept (PoC) of the group NewTechnologies@infologistix (NT) for cloud based replication (cbr) with Confluent cloud, the managed cloud service for Apache Kafka, and Microsoft Azure SQL databases, the database as a service on Microsoft Azure. The replication should be done by change data capture (CDC).
A combination of Confluent Cloud and managed Azure cloud services can be a really good choice for small enterprises as it provides the opportunity to develop serverless applications and therefore pay only for the data usage. That’s not the case when running Apache Kafka at your own data center or using other data streaming services on cloud platforms. And if your business will grow, you still stay flexible: At any point, it is possible to change subscription and get a dedicated cluster with private networking.  For situations that require dedicated clusters or a huge amount of data is shipped, Confluent Platform is probably the cheaper choices than Confluent Cloud.
<img loading="lazy" alt="High level structure of the PoC" src="/assets/images/confluent-kafka-poc-c51f83234cda0b3c305fce7cfd99ec85.png" width="1195" height="356" class="img_ev3q">
<em>Figure 1: High level structure of the PoC</em></p><p>The use case presented here, involves replicating data from a Azure SQL Database source to a Azure SQL Database target by applying the MS SQL server source and sink connector services of Confluent Cloud.  It’s a quit simple case, but a good way to get an impression how easy data replication and the usage of Confluent cloud can be. The whole configuration can be achieved just with the help of the Confluent and Microsoft Azure user interfaces. No coding is needed.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="step-by-step-to-serverless-data-replication">Step by step to serverless data replication<a href="#step-by-step-to-serverless-data-replication" class="hash-link" aria-label="Direct link to Step by step to serverless data replication" title="Direct link to Step by step to serverless data replication">​</a></h3><u> 1. Configuring Azure </u><p>Microsoft Azure offers a variety of managed services built for the cloud. First, let’s see how we can create databases.  For our use case we are going to need a source and a sink database on separate database servers for each one of them.</p><p><img loading="lazy" alt="Azure Portal dashboard" src="/assets/images/azure-portal-dashboard-40cac8a69a69b97b90525755f3768bb2.png" width="627" height="240" class="img_ev3q">
<em>Figure 2: Azure Portal dashboard</em></p><p>The first step is to log into the Azure portal. Then create a resource group. Here apart from the name of the group, the host region must be chosen. From the menu of the resource group, other services including the SQL database can be created and managed. </p><p><img loading="lazy" alt="Create a resource group instance" src="/assets/images/azure-portal-resource-group-aea25592c6240733327c5c3fc66eab06.png" width="644" height="364" class="img_ev3q">
<em>Figure 3: Create a resource group instance</em></p><p>But first an SQL Database server is required. So go back to the home directory and click-on the SQL Database server service. In the create menu, it is asked to assign the server to a valid resource group. The full name of the new server will be the given name plus the suffix “.database.windows.net”. Please keep that in mind for later configurations.  It is preferred to choose the same host location as the one for the resource group. In the authentication section, a username or so-called “Server admin login” and a password must be given. Finally, it is advised to allow the azure services and resources to access your server.</p><p><img loading="lazy" alt="Create an SQL Server instance in the Resource group" src="/assets/images/azure-sql-instance-3d445aa69c17dbf95de8bbdee5074e63.svg" width="654" height="344" class="img_ev3q">
<em>Figure 4: Create an SQL Server instance in the Resource group</em></p><p>Now all the infrastructure to set up the SQL database is available. It can be done at either the SQL server menu or the resource group menu. At the create menu, there are various options regarding  the storage, pricing and performance of the database connection to fit every need. It is again important to allow access to the azure services and to add the client IP address to the firewall. The rest of the networking configurations are meant mostly for advanced users. </p><p><img loading="lazy" alt="Create an SQL database instance in each SQL Server" src="/assets/images/azure-create-sql-database-fc34d3c55400573d41dcc1d4a811c99f.png" width="605" height="277" class="img_ev3q">
<em>Figure 5: Create an SQL database instance in each SQL Server</em></p><p>In this PoC the source SQL server is called  “nt-cbr-sqlserver” whereas the sink server “nt-cbr-sqlserver-dest”. The corresponding source database is called CloudBasedReplication_Source and the sink database “CloudBasedReplication_Target”. The server username is “Infologistix”. This info will be used to configure the confluent cloud connectors.</p><p><img loading="lazy" alt="Resource group Menu" src="/assets/images/azure-resource-group-menu-f303dcefb92cd94a6ea86b650f85d42a.png" width="638" height="337" class="img_ev3q">
<em>Figure 6: Resource group Menu</em></p><p>The database data can be accessed from the query editor inside the database menu or from an external client (e.g SQL Server Management Studio (SSMS)). Activity logs, properties and other information can also be found in the menu. To illustrate the data replication, a table “orderdetails” with the following structure is created in the “CloudBasedReplication_Source” database and is filled with some data:</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#bfc7d5;--prism-background-color:#292d3e"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#bfc7d5"><span class="token plain">CREATE TABLE orderdetails(</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">&quot;ORDER_NUMBER&quot; int identity (1,1) PRIMARY KEY,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">&quot;PRODUCT_CODE&quot; int,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">&quot;QUANTITY_ORDERED&quot; float,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">&quot;PRICE_EACH&quot; decimal(10,5),</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">&quot;CREATED_AT&quot; datetime2 DEFAULT CURRENT_TIMESTAMP NOT NULL)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p><strong>Attention:</strong> The timestamp column must always be of type datetime2. Other formats are not supported.</p><u> 2. Configuring Confluent Cloud </u><p>The initial step in the confluent cloud environment is to set up a cluster. One can choose between three providers: AWS, Google Cloud and Azure. Obviously, for this use case we are going to choose the Azure provider. Furthermore, the host region and the subscription type must be chosen. A basic subscription is enough in this case. For new accounts, a free trial of 2 months and consumption up to 400 US Dollars is offered. </p><p><img loading="lazy" alt="Create Cluster page" src="/assets/images/cluster-page-a00a907dd5083a769050f8b6c01e8af2.png" width="639" height="317" class="img_ev3q">
<em>Figure 7: Create Cluster page</em></p><p><img loading="lazy" alt="Confluent Cloud Cluster dashboard" src="/assets/images/cluster-dashboard-31c7b5113e2d5f0437cf8e1a648ec450.png" width="756" height="338" class="img_ev3q">
<em>Figure 8: Confluent Cloud Cluster dashboard</em></p><p>Next, a schema at the environment dashboard needs to be defined. The Avro schema should be chosen.
Now, click-on the cluster menu. The first thing is to create an API Key to authenticate the cluster. The API key username and password should be saved during its generation. Afterwards there is no way to retrieve the password. </p><p><img loading="lazy" alt="Schema Registry overview" src="/assets/images/schema-registry-overview-c81bf17792c7e9d7115fbb46a6bfe772.png" width="659" height="271" class="img_ev3q">
<em>Figure 9: Schema Registry overview</em></p><p><img loading="lazy" alt="Create API-Key page" src="/assets/images/api-key-page-82f2887fd3411c694a24b0e4194197ce.png" width="673" height="266" class="img_ev3q">
<em>Figure 10: Create API-Key page</em></p><p>Now it is time to set up the SQL server source and sink connectors. There is a huge variety of connectors, so it is easier to use the search bar. Each field in the configuration interface has a sufficient explanation, but there are some details that need to be treated with more care.   </p><p><img loading="lazy" alt="Kafka Connector list" src="/assets/images/kafka-connector-list-e9ed0dfe42568f63349668cd88c44447.png" width="1498" height="603" class="img_ev3q">
<em>Figure 11: Kafka Connector list</em></p><u>Source Connector</u><p>First we will add and configure the source connector:
<img loading="lazy" alt="Add connector page" src="/assets/images/kafka-add-connector-0ae90d790f07ee8358a627939a10ea42.png" width="629" height="288" class="img_ev3q">
<em>Figure 12:  Add connector page</em></p><p>The following screenshot shows the source connector’s configuration used in this example. Only the fields with a given value are visible here. The rest can be left empty, which means they will hold the default value:</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#bfc7d5;--prism-background-color:#292d3e"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#bfc7d5"><span class="token plain">       Source Connector Configuration:</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">{</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  &quot;connector.class&quot;: &quot;MicrosoftSqlServerSource&quot;,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  &quot;name&quot;: &quot;MicrosoftSqlServerSourceConnector_0&quot;,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  &quot;kafka.api.key&quot;: &quot;****************&quot;,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  &quot;kafka.api.secret&quot;: &quot;****************************************************************&quot;,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  &quot;topic.prefix&quot;: &quot;src&quot;,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  &quot;connection.host&quot;: &quot;nt-cbr-sqlserver.database.windows.net&quot;,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  &quot;connection.port&quot;: &quot;1433&quot;,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  &quot;connection.user&quot;: &quot;Infologistix@nt-cbr-sqlserver&quot;,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  &quot;db.name&quot;: &quot;CloudBasedReplication_Source&quot;,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  &quot;table.whitelist&quot;: [</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    &quot;orderdetails&quot;</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  ],</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  &quot;timestamp.column.name&quot;: [</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    &quot;CREATED_AT&quot;</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  ],</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  &quot;incrementing.column.name&quot;: &quot;&quot;,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  &quot;table.types&quot;: [</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    &quot;TABLE&quot;</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  ],</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  &quot;db.timezone&quot;: &quot;Europe/Brussels&quot;,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  &quot;numeric.mapping&quot;: &quot;best_fit&quot;,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  &quot;output.data.format&quot;: &quot;AVRO&quot;,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  &quot;tasks.max&quot;: &quot;1&quot;</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>The name and the login credentials of the source database and server, which were created in Azure, must be given here. The example names are used to show you how to fill in the blanks correctly. The source connector creates automatically a topic with the prefix “src” plus the initial table name “orderdetails”, which serializes the table data with the Avro schema and acts like a Kafka producer. One should be careful with the prefix and table names, as specific characters might cause connector failure. The bad thing is that the confluent error messages
might not identify the true cause of this problem. Underscores and dots are prohibited, whereas dash is allowed.</p><u>Sink connector</u><p>When adding the sink connector, proceed in the same way as for the source connector.</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#bfc7d5;--prism-background-color:#292d3e"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#bfc7d5"><span class="token plain">   Sink Connector Configuration:</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">{</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  &quot;topics&quot;: [</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    &quot;srcorderdetails&quot;</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  ],</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  &quot;input.data.format&quot;: &quot;AVRO&quot;,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  &quot;connector.class&quot;: &quot;MicrosoftSqlServerSink&quot;,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  &quot;name&quot;: &quot;MicrosoftSqlServerSinkConnector_0&quot;,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  &quot;kafka.api.key&quot;: &quot;****************&quot;,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  &quot;kafka.api.secret&quot;: &quot;****************************************************************&quot;,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  &quot;connection.host&quot;: &quot;nt-cbr-sqlserver-dest.database.windows.net&quot;,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  &quot;connection.port&quot;: &quot;1433&quot;,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  &quot;connection.user&quot;: &quot;Infologistix@nt-cbr-sqlserver-dest&quot;,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  &quot;db.name&quot;: &quot;CloudBasedReplication_Target&quot;,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  &quot;insert.mode&quot;: &quot;INSERT&quot;,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  &quot;db.timezone&quot;: &quot;Europe/Brussels&quot;,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  &quot;pk.mode&quot;: &quot;none&quot;,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  &quot;auto.create&quot;: &quot;true&quot;,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  &quot;auto.evolve&quot;: &quot;true&quot;,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  &quot;batch.sizes&quot;: &quot;1000&quot;,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  &quot;tasks.max&quot;: &quot;1&quot;</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>The name and the login credentials of the sink database need to be given here. Input format is AVRO. The sink connector consumes data from the topic “srcorderdetails” and load them into a table in the target database with the same name. Messages which cannot be loaded into the target are transferred it into a newly created Dead Letter Topic (in our case “dlq-lcc-50d8z”). Since the “auto.create” field is true, the table in the target Database is automatically created.
This should be the overview of the connector and topic sections by now. If the schema does not seem to be set for a topic, it can be manually set:</p><p><img loading="lazy" alt="Topics dashboard after all connections are set" src="/assets/images/topics-1143031b26ccbca42bff135fdd1e3710.png" width="852" height="203" class="img_ev3q">
<em>Figure 13: Topics dashboard after all connections are set</em></p><p>First we will add and configure the source connector:
<img loading="lazy" alt="Connector’s dashboard  after all connections are set" src="/assets/images/connectors-d6ecca0775d94aa8f04553dcdcfc80c6.png" width="845" height="202" class="img_ev3q">
<em>Figure 14: Connector’s dashboard  after all connections are set</em></p><p>The data flow can be seen at section “Stream lineage” as below:</p><p><img loading="lazy" alt="Data flow" src="/assets/images/flow-0acb116d17cd2ec9f3e80d849f1e7f7a.png" width="817" height="233" class="img_ev3q">
<em>Figure 15: Data flow</em></p><p>Data are now present in the sink database:</p><p>First we will add and configure the source connector:
<img loading="lazy" alt="View newly transferred data in query editor  " src="/assets/images/transferred-data-f1769edc53318e7e594f33620702d6a8.png" width="756" height="307" class="img_ev3q">
<em>Figure 16: View newly transferred data in query editor</em></p><u>Troubleshooting</u><p>Sometimes the connectors fail to run at first. If the error is unexpected then deleting and recreating can help. It can also happen that the error disappears after a couple of minutes by itself.</p><u>A few remarks about cdc connectors</u><p>Now, every time new data arrives at the source database, they will be directly processed and stored at the sink database. It should be noted though, that this is true only for rows with a timestamp value greater than the one of the last fetch. If more than one timestamp columns is defined in the source connector configuration, then the first non-null value will be considered. Same applies to modified rows. They are simply dealt as new rows, rather than updated.</p><p><strong>Info:</strong> The SQL server connectors are constantly upgraded to offer more options to the user. </p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a href="#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h3><p>Our proof of concept was able to show that it is extremely easy to use the functionality of Apache Kafka with the Confluent Cloud. A data replication for a table could be realized in a few minutes. However, the Confluent Cloud also offers the possibility to perform transformations through the integration of KSQL, so that ETL operations are also possible. In addition, this also enables problem-free changes of the SQL dialect, since date formats can be adapted through simple transformations. Scenarios in which an on premise database (e.g. IBM DB2 or Oracle DB) is replicated via confluent cloud and is then backed up in the Azure Cloud are therefore also no problem.
We hope we have been able to give you a small insight into the possibilities of managed cloud services from our partners Confluent and Microsoft and make it easier for you to get started with cloud computing.</p></div><footer class="row docusaurus-mt-lg blogPostFooterDetailsFull_mRVl"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/confluent">Confluent</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/kafka">Kafka</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/azure">Azure</a></li></ul></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Blog post page navigation"><a class="pagination-nav__link pagination-nav__link--prev" href="/blog/lecture"><div class="pagination-nav__sublabel">Newer Post</div><div class="pagination-nav__label">First Lecture of Cloud Native Computing</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/blog/docker-in-wsl"><div class="pagination-nav__sublabel">Older Post</div><div class="pagination-nav__label">Docker in WSL</div></a></nav></main><div class="col col--2"><div class="tableOfContents_bqdL thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#introduction" class="table-of-contents__link toc-highlight">Introduction</a></li><li><a href="#step-by-step-to-serverless-data-replication" class="table-of-contents__link toc-highlight">Step by step to serverless data replication</a></li><li><a href="#conclusion" class="table-of-contents__link toc-highlight">Conclusion</a></li></ul></div></div></div></div></div><footer class="footer"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="margin-bottom--sm"><a href="https://infologistix.de/" rel="noopener noreferrer" class="footerLogoLink_BH7S"><img src="/./img/infologistix_logo.png" alt="infologistix logo" class="themedImage_ToTc themedImage--light_HNdA footer__logo" width="82" height="48" style="float:right"><img src="/./img/infologistix_logo.png" alt="infologistix logo" class="themedImage_ToTc themedImage--dark_i4oU footer__logo" width="82" height="48" style="float:right"></a></div><div class="footer__copyright">Copyright © 2024 <a href="https://infologistix.de/">infologistix GmbH</a>, Inc. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.565f4334.js"></script>
<script src="/assets/js/main.1ec76ff8.js"></script>
</body>
</html>