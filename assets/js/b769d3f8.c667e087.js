"use strict";(self.webpackChunkkubernetes=self.webpackChunkkubernetes||[]).push([[52],{3905:(e,n,t)=>{t.d(n,{Zo:()=>d,kt:()=>g});var r=t(7294);function i(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function s(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);n&&(r=r.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,r)}return t}function a(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?s(Object(t),!0).forEach((function(n){i(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):s(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function l(e,n){if(null==e)return{};var t,r,i=function(e,n){if(null==e)return{};var t,r,i={},s=Object.keys(e);for(r=0;r<s.length;r++)t=s[r],n.indexOf(t)>=0||(i[t]=e[t]);return i}(e,n);if(Object.getOwnPropertySymbols){var s=Object.getOwnPropertySymbols(e);for(r=0;r<s.length;r++)t=s[r],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(i[t]=e[t])}return i}var u=r.createContext({}),o=function(e){var n=r.useContext(u),t=n;return e&&(t="function"==typeof e?e(n):a(a({},n),e)),t},d=function(e){var n=o(e.components);return r.createElement(u.Provider,{value:n},e.children)},c="mdxType",m={inlineCode:"code",wrapper:function(e){var n=e.children;return r.createElement(r.Fragment,{},n)}},h=r.forwardRef((function(e,n){var t=e.components,i=e.mdxType,s=e.originalType,u=e.parentName,d=l(e,["components","mdxType","originalType","parentName"]),c=o(t),h=i,g=c["".concat(u,".").concat(h)]||c[h]||m[h]||s;return t?r.createElement(g,a(a({ref:n},d),{},{components:t})):r.createElement(g,a({ref:n},d))}));function g(e,n){var t=arguments,i=n&&n.mdxType;if("string"==typeof e||i){var s=t.length,a=new Array(s);a[0]=h;var l={};for(var u in n)hasOwnProperty.call(n,u)&&(l[u]=n[u]);l.originalType=e,l[c]="string"==typeof e?e:i,a[1]=l;for(var o=2;o<s;o++)a[o]=t[o];return r.createElement.apply(null,a)}return r.createElement.apply(null,t)}h.displayName="MDXCreateElement"},969:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>u,contentTitle:()=>a,default:()=>m,frontMatter:()=>s,metadata:()=>l,toc:()=>o});var r=t(7462),i=(t(7294),t(3905));const s={slug:"webcrawler",title:"Use Case f\xfcr einen WebCrawler",tags:["Python3","Azure"]},a=void 0,l={permalink:"/kubernetes/blog/webcrawler",source:"@site/blog/2021-07-12-webcrawler.md",title:"Use Case f\xfcr einen WebCrawler",description:"WebCrawler sind eine einfache, effektive und kosteng\xfcnstige M\xf6glichkeit Websiten gezielt nach Informationen zu durchsuchen und Ihnen komprimiert zur Verf\xfcgung zu stellen. Die Programme sind damit ideal daf\xfcr geeignet repetitive Aufgaben zu erledigen.",date:"2021-07-12T00:00:00.000Z",formattedDate:"July 12, 2021",tags:[{label:"Python3",permalink:"/kubernetes/blog/tags/python-3"},{label:"Azure",permalink:"/kubernetes/blog/tags/azure"}],readingTime:7.49,hasTruncateMarker:!0,authors:[],frontMatter:{slug:"webcrawler",title:"Use Case f\xfcr einen WebCrawler",tags:["Python3","Azure"]},prevItem:{title:"Docker in WSL",permalink:"/kubernetes/blog/docker-in-wsl"}},u={authorsImageUrls:[]},o=[{value:"WebCrawler - Use Case",id:"webcrawler---use-case",level:3},{value:"Anwendungsbeispiel",id:"anwendungsbeispiel",level:4},{value:"Res\xfcmee",id:"res\xfcmee",level:4},{value:"WebCrawler - How-To",id:"webcrawler---how-to",level:3},{value:"Was man daf\xfcr ben\xf6tigt",id:"was-man-daf\xfcr-ben\xf6tigt",level:4},{value:"F\xfcr die Anwendung in der Cloud wahlweise:",id:"f\xfcr-die-anwendung-in-der-cloud-wahlweise",level:4},{value:"Legen wir los",id:"legen-wir-los",level:4}],d={toc:o},c="wrapper";function m(e){let{components:n,...t}=e;return(0,i.kt)(c,(0,r.Z)({},d,t,{components:n,mdxType:"MDXLayout"}),(0,i.kt)("p",null,"WebCrawler sind eine einfache, effektive und kosteng\xfcnstige M\xf6glichkeit Websiten gezielt nach Informationen zu durchsuchen und Ihnen komprimiert zur Verf\xfcgung zu stellen. Die Programme sind damit ideal daf\xfcr geeignet repetitive Aufgaben zu erledigen."),(0,i.kt)("p",null,"In diesem Artikel stellen wir Ihnen einen Use Case f\xfcr einen WebCrawler vor und geben Ihnen ein ausf\xfchrliches How-To zur Einrichtung eines cloudbasierten Dockers."),(0,i.kt)("h3",{id:"webcrawler---use-case"},"WebCrawler - Use Case"),(0,i.kt)("p",null,"WebCrawler sind leichtgewichtige und kosteneffiziente Datensammler, die Ihnen einen Informationsvorsprung sichern.\nHeutzutage sind (fast) alle Informationen online verf\xfcgbar. Googeln ist ein Synonym f\xfcr das Finden von Informationen geworden, vom einfachen Kochrezept bis hin zur wissenschaftlichen Abhandlung."),(0,i.kt)("p",null,"Die meisten dieser Daten sind frei zug\xe4nglich und k\xf6nnen einfach abgerufen werden. Die schiere Menge an Daten und Websites macht eine Suche aber sehr zeit- und personalintensiv. Der Zeitaufwand spielt insbesondere dann eine Rolle, wenn die Suche nicht nur einmalig geschehen soll, sondern regelm\xe4\xdfig, um dann die \xc4nderungen und Entwicklungen zu monitoren. Hier bieten sich sogenannte WebCrawler als eine einfache und vor allem kosteneffiziente M\xf6glichkeit an, die Suche zu automatisieren."),(0,i.kt)("p",null,"WebCrawler sind Programme, die automatisiert das Internet durchsuchen und Websites analysieren. Sie sind damit perfekt geeignet, um repetitive Aufgaben zu erledigen."),(0,i.kt)("h4",{id:"anwendungsbeispiel"},"Anwendungsbeispiel"),(0,i.kt)("p",null,"Bei der infologistix GmbH verwenden wir WebCrawler f\xfcr verschiedene Aufgaben, u.a. um \xf6ffentliche Ausschreibungsseiten zu monitoren und die f\xfcr uns interessanten Ausschreibungen heraus zu filtern. Dazu crawlen wir ca. zwei Dutzend verschiedene Websites von Beh\xf6rden, Unternehmen und Portalen. Crawlen bedeutet hier, dass ein cloud-basierter ",(0,i.kt)("u",null,"Docker")," die Seiten nacheinander aufruft, sie nach interessanten Ausschreibungen durchsucht und uns eine Ergebnisliste mit ausschlie\xdflich relevanten Ausschreibungen zur Verf\xfcgung stellt. Technische Details, sowie das Dockerimage und eine Anleitung zum Nachbauen finden Sie im folgenden ",(0,i.kt)("a",{parentName:"p",href:"#webcrawler---how-to"},"How-To Abschnitt"),"."),(0,i.kt)("p",null,"Die Ergebnisliste mit relevanten Ausschreibungen stellt f\xfcr uns eine enorme Zeit- und Kostenersparnis dar. Anstatt hunderte von Ausschreibungen pro Tag durchsuchen zu m\xfcssen, m\xfcssen jetzt nur noch ein halbes Dutzend Ausschreibung gepr\xfcft werden. Und das bei nur ca. 1\u20ac Betriebskosten pro Woche f\xfcr den WebCrawler."),(0,i.kt)("h4",{id:"res\xfcmee"},"Res\xfcmee"),(0,i.kt)("p",null,"WebCrawler sind eine einfache, effektive und kosteng\xfcnstige M\xf6glichkeit, Websites gezielt nach Informationen zu durchsuchen und Ergebnisse komprimiert zur Verf\xfcgung zu stellen. Unser Dockerimage stellt da eine besonders leichtgewichtige und easy to-use Variante dar. Sollten Sie Fragen zu WebCrawlern haben oder eine ma\xdfgeschneiderte L\xf6sung suchen, kontaktieren Sie uns gerne."),(0,i.kt)("h3",{id:"webcrawler---how-to"},"WebCrawler - How-To"),(0,i.kt)("p",null,"Mit diesem How-To zeigen wir, wie man einen Cloud-basierten Docker einrichtet, um damit Websites nach Informationen zu durchsuchen und aufbereitete Suchergebnisse mittels Benachrichtigung \xfcber Social Chat wie MS Teams oder Slack oder auch E-Mail zur Verf\xfcgung zu stellen."),(0,i.kt)("h4",{id:"was-man-daf\xfcr-ben\xf6tigt"},"Was man daf\xfcr ben\xf6tigt"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Lokale Docker Runtime"),(0,i.kt)("li",{parentName:"ul"},"Social Chat (MS Teams, Slack) oder E-Mail Client"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://hub.docker.com/r/infologistix/docker-selenium-python"},"infologistix WebCrawler von Docker Hub")),(0,i.kt)("li",{parentName:"ul"},"Etwas Erfahrung mit ",(0,i.kt)("a",{parentName:"li",href:"https://www.python.org/"},"Python"),", Command Line, HTML")),(0,i.kt)("h4",{id:"f\xfcr-die-anwendung-in-der-cloud-wahlweise"},"F\xfcr die Anwendung in der Cloud wahlweise:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://azure.microsoft.com/en-us/"},"Microsoft Azure")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://aws.amazon.com/"},"Amazon AWS")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://cloud.google.com/"},"Google Cloud Platform"))),(0,i.kt)("h4",{id:"legen-wir-los"},"Legen wir los"),(0,i.kt)("u",null," 0. GitHub Repository Klonen "),(0,i.kt)("p",null,"Dieses How-To basiert auf einer Projektstruktur, welche wir bereits angelegt haben und, die Sie direkt in Ihr Projekt \xfcbernehmen k\xf6nnen.\nUnter dem folgenden Link finden Sie dieses How-To auch ",(0,i.kt)("strong",{parentName:"p"},"als fertiges Programm im Ordner Examples.")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"$ git clone https://github.com/infologistix/docker-selenium-python.git ./infologistix\n")),(0,i.kt)("p",null,"Wir wollen hier ein einfaches Beispiel betrachten und zun\xe4chst herausfinden, welche Services unser Unternehmen bietet. Die hier gezeigten Funktionen lassen sich einfach auf neue Gegebenheiten anpassen"),(0,i.kt)("u",null," 1. Grundstruktur und Bibliotheken "),(0,i.kt)("p",null,"Das Test-Framework Selenium bietet uns bereits eine gro\xdfe Auswahl an Funktionen und Equipment, um Websites gezielt zu bearbeiten und Informationen zu ermitteln. Die Grundstruktur unseres Projektes basiert auf einer Klasse, welche die Kommunikation mit dem Browser \xfcbernimmt und uns s\xe4mtliche Daten aus der Webseite extrahiert. Dar\xfcber hinaus verwenden wir pandas als Tool f\xfcr die Formatierung und Datenanalyse der Suchergebnisse."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"from selenium.webdriver import Chrome\nfrom selenium.webdriver import ChromeOptions \nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support import expected_conditions as EC\nimport pandas as pd\n")),(0,i.kt)("p",null,"Ausgehend von den genutzten Bibliotheken k\xf6nnen wir eine Basisklasse f\xfcr unseren Crawler schreiben. Diese Basisklasse beschreibt den allgemeinen Crawler. Wir verwenden die Klassendarstellung, da hier die Website und deren Funktionen und Elementextraktion als Variablen und Funktionen aufgerufen werden k\xf6nnen. Initial \xf6ffnen wir hiermit ein Chrome-Fenster im \u201eheadless\u201c-Modus und \xf6ffnen die gegebene Webseite."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},'class InfologistixCrawler():\n    def __init__(self, url: str, headless: bool=True) -> None:\n        options = ChromeOptions()\n        options.add_argument("\u2013no-sandbox")\n        options.add_argument("\u2013window-size=1280,720")\n        if headless:\n            options.add_argument("\u2013headless")\n        self.driver = Chrome(options=options)\n        self.driver.get(url)\n\n    def run(self):\n        page = self.driver.page_source\n        self.close()\n        return page\n\n    def close(self):\n        self.driver.close()\n')),(0,i.kt)("u",null," 2. Informationen der Webseite ermitteln "),(0,i.kt)("p",null,"Wir filtern aus unserer eigenen ",(0,i.kt)("a",{parentName:"p",href:"https://infologistix.de/"},"Homepage")," die angebotenen Dienstleistungen heraus und wollen diese mit Namen, Details und Referenz abspeichern. Dazu suchen wir uns die Informationen auf der Website und ermitteln die HTML-Grundstruktur. In unserem Fall befinden sich die Dienstleistungen in einem \u201asection\u2018-Element, welches wir finden m\xfcssen. Unser gesuchtes Element besitzt die ID \u201aLeistungen\u2018, welches wiederum mehrere \u201asection\u2018-Elemente mit dem Klassenattribut \u201aelementor-image-box-content\u2018 beinhaltet. Hier sind alle Dienstleistungen hinterlegt."),(0,i.kt)("p",null,"Zun\xe4chst warten wir, bis das gesuchte Element vorhanden ist. Wir teilen mit WebDrverWait unserem Programm mit, maximal 10 Sekunden darauf zu warten, dass unser gefordertes Element auf der Webseite im DOM vorhanden ist. Ist dieses Element nicht vorhanden, dann wird hier aus dem laufenden Programm ausgestiegen und man l\xe4uft in keine Fehler."),(0,i.kt)("p",null,"Dann speichern wir uns den Container mit den gesuchten Elementen mittels der find_element_by","*","-Funktion und suchen innerhalb dieses Containers mit find_elements_by* alle gesuchten Elemente und extrahieren die einzelnen Informationen der Extraktion."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},'def getServices(self) -> list:\n        results = list()\n        WebDriverWait(self.driver, 10).until(EC.presence_of_element_located((By.ID, "Leistungen")))\n        services = self.driver.find_element_by_id ("Leistungen")\n        for service in services.find_elements_by_tag_name("section"):\n            results.append(\n                 self.__extract(\n                          service.find_element_by_class_name(\n                                   "elementor-image-box-content")))\n\n         return results\n')),(0,i.kt)("p",null,"Als Ergebnis erhalten wir eine Liste, welche wir nur noch zur\xfcckgeben m\xfcssen. Was genau die Funktion __extract() macht, das erkl\xe4ren wir gleich"),(0,i.kt)("p",null,"Mit Hilfe der Liste l\xe4sst sich jetzt eine Darstellung der einzelnen Leistungen entwickeln, welche die Sichtbarkeit und Lesbarkeit f\xfcr den Menschen vereinfacht. Wir w\xe4hlen hierbei die Darstellung eines Pandas DatenFrames, da hier z.B. auch numerische Berechnungen und Aggregationen vorgenommen werden k\xf6nnen. Zus\xe4tzlich k\xf6nnen hieraus auch Excel-Dateien erstellt werden. Wir bilden hier also einen DataFrame mit den Spalten \u201eURI\u201c, \u201eTitle\u201c und \u201eDescription\u201c aus unserer Liste, in welchem der Titel, Beschreibung und die jeweilige Referenz steht."),(0,i.kt)("p",null,"Zur\xfcckgegeben wird dann ein DatenFrame."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"def makeFrame(self, services: list) -> pd.DataFrame:\n    return pd.DataFrame(services)\n")),(0,i.kt)("p",null,"Auf Basis dieser Transformation der Ergebnisse stellen wir unsere run() Funktion um, sodass sie uns die gefundenen Ergebnisse liefert und die erstellten Funktionen einbindet."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"def run(self):\n     services = self.getServices()\n     self.close()\n     return self.makeFrame(services)\n")),(0,i.kt)("p",null,"Erinnern wir uns zur\xfcck an unsere __extract() Funktion. Hier wollen wir aus den Elementen die erforderlichen Informationen extrahieren. Relevante Informationen herauszufinden und zu filtern, ist hier die Hauptaufgabe. S\xe4mtliche Informationen sind in unterschiedlichen Elementen mit unterschiedlichen Identifizierungsm\xf6glichkeiten vorhanden. Der Titel steht zum Beispiel als Text innerhalb eines \u201aa\u2018-Elements, welches zus\xe4tzlich noch die Referenz beinhaltet. In unserem Beispiel setzt sich die Extraktionsfunktion wie folgt zusammen und gibt uns ein geordnetes Dictionary zur\xfcck, in welchem die Elementinformationen enthalten sind."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},'def __extract(self, service: WebElement) -> dict:\n        return {\n           "URI": service.find_element_by_tag_name("a").get_attribute("href"),\n           "Title": service.find_element_by_tag_name("a").text,\n           "Description": service.find_element_by_tag_name("p").text,\n         }\n')),(0,i.kt)("p",null,"Die gesamte Klasse mit Funktionen sieht dann wie folgt aus:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},'class InfologistixCrawler():\n    def __init__(self, url, headless=False):\n        options = ChromeOptions()\n        options.add_argument("\u2013no-sandbox")\n        options.add_argument("\u2013window-size=1280,720")\n        if headless:\n            options.add_argument("\u2013headless")\n        self.driver = Chrome(options=options)\n        self.driver.get(url)\n\n    def getServices(self) -> list:\n        results = list()\n        WebDriverWait(self.driver, 10).until(EC.presence_of_element_located((By.ID, "Leistungen")))\n        services = self.driver.find_element_by_id ("Leistungen")\n        for service in services.find_elements_by_tag_name("section"):\n            results.append(\n                 self.__extract(\n                          service.find_element_by_class_name(\n                                   "elementor-image-box-content")))\n        return results\n\n    def __extract(self, service: WebElement) -> dict:\n        return {\n           "URI": service.find_element_by_tag_name("a").get_attribute("href"),\n           "Title": service.find_element_by_tag_name("a").text,\n           "Description": service.find_element_by_tag_name("p").text,\n         }\n\n    def makeFrame(self, services : list) -> pd.DataFrame:\n        return pd.DataFrame(services)\n\n    def run(self) -> pd.DataFrame:\n        services = self.getServices()\n        self.close()\n        return self.makeFrame(services)\n\n    def close(self) -> None:\n        self.driver.close()\n')),(0,i.kt)("p",null,"Lassen wir unseren Crawler laufen, dann erhalten wir einen DataFrame mit den angebotenen Leistungen der infologistix GmbH."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},'services = Crawler(url="https://infologistix.de").run()\n')),(0,i.kt)("p",null,"Wir k\xf6nnen nun die Dienstleistungen der infologistix GmbH von der Website extrahieren und als Dataframe ausgeben lassen. Der n\xe4chste Schritt ist die \xdcbermittlung der Ergebnisse."),(0,i.kt)("u",null," 3. \xdcbermittlung & Messaging "),"F\xfcr Teams, als auch Slack wird ein Token beziehungsweise Webhook verwendet, um eine formatierte Nachricht an einen Channel zu senden. Die Integration von Slack ist leider noch nicht vollst\xe4ndig umgesetzt und kann daher zu Fehlern f\xfchren.",(0,i.kt)("p",null,(0,i.kt)("a",{parentName:"p",href:"https://dev.outlook.com/Connectors/GetStarted#creating-messages-through-office-365-connectors-in-microsoft-teams"},"Hier findest du eine detaillierte Anleitung, wie man an eine Webhook eines Kanals in MS Teams gelangt.")),(0,i.kt)("p",null,"Hier benutzen wir die Basisbibliothek pymsteams und erstellen eine Card mit der uns verf\xfcgbaren Webhook URL. Wir f\xfcgen dem Beitrag einen Titel und die als HTML formatierten Suchergebnisse an und schicken ihn an unseren Teams-Channel."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},'import pymsteams\n\nmessage = services.to_html()\ntitle = "Dienstleistungen Infologistix"\n\ndef sendMSTeams(webhook : str, message : str, title : str) -> Literal[True]:\n    channel = pymsteams.connectorcard(webhook)\n    channel.title(title)\n    channel.text(message)\n    return channel.send()\n')),(0,i.kt)("p",null,"Wir haben nun unseren Ablauf geplant und k\xf6nnen diesen als main.py speichern."),(0,i.kt)("u",null," 4. Cloud-Container "),"Mit dem hier erstellten Script k\xf6nnen wir jetzt einen Docker-Container erstellen, welcher uns die M\xf6glichkeit gibt, den Crawler in der Cloud laufen zu lassen. Hierzu erstellen wir das Dockerfile oder nehmen das bereits vorhandene Beispiel auf.",(0,i.kt)("p",null,"Es reicht hier ein einfacher Befehl um dieses Beispiel laufen zu lassen."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"$ docker run -it \u2013rm \u2013shm-size=128m docker-selenium-python:latest python ./examples/main.py\n")),(0,i.kt)("p",null,"Eine Anleitung f\xfcr das Einrichten in der Cloud-Umgebung findest du hier:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://learn.microsoft.com/de-de/azure/container-instances/container-instances-tutorial-prepare-app"},"Microsoft Azure")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://us-east-1.signin.aws.amazon.com/oauth?SignatureVersion=4&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=ASIAQHJ3VXZJKWI3EBO4&X-Amz-Date=2023-05-15T15%3A08%3A10.140Z&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEO%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCIQChMDyBnLd2%2F6pRONs4Qt%2ByvBbsOpAnL9akqgT%2FbmPzHQIgQDYigveW%2FZwaS10R4VZkdtTA08gEFv%2FXedWIz5r%2ByhkqzQMIGBACGgwwMTU2OTQ0MTMzOTQiDHXEpnkLczxPEFM%2B1SqqAzRfwLNltRQTxPe6NJdMLiwSKoLDoKdz0cfX%2B3V1qQ9SLmVKLxC3Ar3a7JNJzQ2P%2Fj7nrb5FGWM2FhQ8GUt6Zr1YE%2F%2FqvVm1%2FESeKDWHlsJMg0fF8IxeY9vQSVPE59zHD4IP94QeLfiHQPY0zO8fIY98FH6SBw18a%2FJh4AbGv20QF92lXwqVVJlmeyfdKdOTP9%2FjPwAZV1F8FItFqkqkOb8r8p8QwvPEDPr%2B0CqOdOQ631qdOtT%2FBYsOcXcvqBxeAyx1ud2SsvHC1bxG7eFdUGPrigT%2B08LU6rWtabEzNFFsVHzYV45K4o56JTO8%2BeFeKgri7N5qAINjmWAKzBRpSvF2zLx8i5Y%2BLYLx278yZwaNRSTCj%2FzaDkI%2BSP1cqU2qLStDbjXD51CrA3RSVxMjUn7%2BqqF48ynxogK2gsPeNAXSpt4RUwen28SINrnzw6ajQHxU3bxKa8pHx4sIe%2Fq6%2BmsHNdwP9EJsQ5WT0Dsd16F29IVrSfOPbN7azqPuCmVT5HmXtC13WsltPEHL4LHYlGCNqWQy2yIe0ivXLdoEmTEl1aIzwUpWCi0uEjDUgYmjBjqhAYubtfW9U3AJzxe55%2FXr2Ps%2FTiqGV8tWTNhFESUbyVKKXPXeFrbvrQd%2BXNUhXekARqS4aqbyiqmNxy3HyoxWFXnQkIrZHNZfj8kORXtj6QpWwGtYi7J5kh9TOAyW1GASQpj5CLuk8LnvsBLsAqmdSmuxD69GwoaZ1%2FqcSd441OA1uIiaYjeVM4MAU7Z%2F5hKmgLFacJZwD7UaHq7BNIjt2YZ8&X-Amz-Signature=88941b4e1ab60b16a86d29aa30169819c908f3b5558268f10040ef6c273940e1&X-Amz-SignedHeaders=host&client_id=arn%3Aaws%3Aiam%3A%3A015428540659%3Auser%2Fecs&code_challenge=Rzvs-__JFcYEHVU8L8WkBiXx4NhaNcxiZN2krlbe1XQ&code_challenge_method=SHA-256&redirect_uri=https%3A%2F%2Fus-east-1.console.aws.amazon.com%2Fecs%2Fhome%3Fregion%3Dus-east-1%26state%3DhashArgs%2523%252FgetStarted%26isauthcode%3Dtrue&region=us-east-1&response_type=code&state=hashArgs%23%2FgetStarted"},"Amazon AWS")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://cloud.google.com/container-registry/docs/pushing-and-pulling?hl=de"},"Google Cloud Services"))))}m.isMDXComponent=!0}}]);