"use strict";(self.webpackChunkkubernetes=self.webpackChunkkubernetes||[]).push([[145],{9669:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"testdata","metadata":{"permalink":"/kubernetes/blog/testdata","source":"@site/blog/2022-10-20-testdata.md","title":"Testdaten Sampling","description":"Bevor neue Applikationen in Produktion gehen, sind datengetriebene Tests unabdingbar, um die Qualit\xe4t von Software und Anwendungen sicherzustellen. F\xfcr diese Tesings haben wir einen Testdatengenerator gebaut.","date":"2022-10-20T00:00:00.000Z","formattedDate":"20. Oktober 2022","tags":[{"label":"Python3","permalink":"/kubernetes/blog/tags/python-3"},{"label":"Azure","permalink":"/kubernetes/blog/tags/azure"}],"readingTime":1.38,"hasTruncateMarker":true,"authors":[{"name":"Marie Padberg","title":"Data Scientist","url":"https://github.com/MariePad","imageURL":"https://github.com/MariePad.png","key":"padberg"}],"frontMatter":{"slug":"testdata","title":"Testdaten Sampling","authors":["padberg"],"tags":["Python3","Azure"]},"nextItem":{"title":"Docker in WSL","permalink":"/kubernetes/blog/docker-in-wsl"}},"content":"Bevor neue Applikationen in Produktion gehen, sind datengetriebene Tests unabdingbar, um die Qualit\xe4t von Software und Anwendungen sicherzustellen. F\xfcr diese Tesings haben wir einen Testdatengenerator gebaut.\\r\\n\x3c!--truncate--\x3e\\r\\n\\r\\nPersonenbezogene Daten in vielen Branchen ein Kernelement der gesamten Gesch\xe4ftst\xe4tigkeit, z.B. bei Versicherungen, Banken und nat\xfcrlich im Handel. Die hier abgelegten sensible und hoch-differenzierten Informationen k\xf6nnen aus datenschutzrechtlichen Gr\xfcnden nicht f\xfcr das Testing genutzt werden. Statt dessen werden oft synthetische Daten genutzt, deren Struktur meist nicht den Originaldatenbestande wiedergeben.\\r\\n\\r\\nHierdurch k\xf6nnen keine realistischen Testbedingungen geschaffen werden. Die Folge sind deutlich h\xf6here Aufwendungen, weil Fixinings im laufenden Betrieb installiert werden m\xfcssen. Im schlimmsten Fall k\xf6nnen sogar Akzeptanzprobleme bei Nutzern und Kunden entstehen.\\r\\n\\r\\n**Notwendig sind stattdessen Testdaten, die realistisch und repr\xe4sentativ f\xfcr die den Gesamtdatenbestand sind.** Die Erstellung solcher Testdaten bedeutet allerdings in vielen F\xe4llen einen hohen Arbeitsaufwand, da Abh\xe4ngigkeiten erhalten bleiben m\xfcssen, die Datentypen sich nicht \xe4ndern d\xfcrfen, Outlier Beachtung brauchen, personenbezogene Daten nicht ohne Pseudonomisierung verwendet werden sollten usw.\\r\\n\\r\\n**Deshalb haben wir einen Testdatengenerator entwickelt, der aus einem gro\xdfen Originaldatenset ein repr\xe4sentatives Sample erstellt und dieses anschlie\xdfend pseudonomisiert.**\\r\\n\\r\\nDabei kann aus zwei unterschiedlichen Samplingmethoden ausgew\xe4hlt werden, welche wir vorab mithilfe statistischer Verfahrensweisen evaluiert haben. Au\xdferdem stehen verschiedene Pseudonomisierungen zur Verf\xfcgung. Zum Schluss wird ein Download der Testdaten und ein kurzer Bericht, mit einer Gegen\xfcberstellung der Original- und Testdaten, zur Verf\xfcgung gestellt.\\r\\n\\r\\nAktuell haben wir das gesamte System mittels Azure Functions als on-demand Website bereitgestellt. Das hei\xdft, bei einer hohen Nachfrage werden mehr Ressourcen so lange wie notwendig bereitgehalten. Bei einem R\xfcckgang der Nachfragen, werden die Ressourcen wieder reduziert. Deshalb kann das Laden der Seite manchmal ein paar Sekunden dauern.\\r\\n\\r\\nAktuell befinden wir uns noch in der Testphase des [Testdatengenerators](https://kitestdataengine.azurewebsites.net/file_upload)."},{"id":"docker-in-wsl","metadata":{"permalink":"/kubernetes/blog/docker-in-wsl","source":"@site/blog/2022-01-27-docker-in-wsl.md","title":"Docker in WSL","description":"Docker Desktop wird kostenplichtig. Wir zeigen Ihnen eine kostenfreie Alternative - Docker im Windows Subsystem for Linux (WSL)","date":"2022-01-27T00:00:00.000Z","formattedDate":"27. Januar 2022","tags":[{"label":"Docker","permalink":"/kubernetes/blog/tags/docker"},{"label":"WSL","permalink":"/kubernetes/blog/tags/wsl"}],"readingTime":4.425,"hasTruncateMarker":true,"authors":[{"name":"Suphanat Avipan","title":"IT Consultant","url":"https://github.com/suphanataviphan","imageURL":"https://github.com/suphanataviphan.png","key":"aviphan"},{"name":"Harald P. Gerhards","title":"Senior IT Consultant | Head of Cloud Engineering","url":"https://github.com/HPG84","imageURL":"https://github.com/HPG84.png","key":"gerhards"},{"name":"Nico Graap","title":"IT Consultant","url":"https://github.com/Nico-infologistix","imageURL":"https://github.com/Nico-infologistix.png","key":"graap"},{"name":"Paul Schmidt","title":"Data Scientist","url":"https://github.com/pickmylight","imageURL":"https://github.com/pickmylight.png","key":"schmidt"}],"frontMatter":{"slug":"docker-in-wsl","title":"Docker in WSL","authors":["aviphan","gerhards","graap","schmidt"],"tags":["Docker","WSL"]},"prevItem":{"title":"Testdaten Sampling","permalink":"/kubernetes/blog/testdata"},"nextItem":{"title":"Use Case f\xfcr einen WebCrawler","permalink":"/kubernetes/blog/webcrawler"}},"content":"Docker Desktop wird kostenplichtig. Wir zeigen Ihnen eine kostenfreie Alternative - Docker im Windows Subsystem for Linux (WSL)\\r\\n\x3c!--truncate--\x3e\\r\\n\\r\\nDie kostenfreien Produkte von Docker werden von Millionen Entwicklern verwendet, um Anwendungen zu erstellen, zu ver\xf6ffentlichen und auszuf\xfchren \u2013 in Rechenzentren, der public cloud oder mit Docker Desktop auf dem lokalen PC. 55% der Entwickler nutzen Docker jeden Tag bei der Arbeit.\\r\\n\\r\\n#### Docker Desktop wird kostenpflichtig\\r\\nDoch die Firma Docker muss auch wirtschaftlich arbeiten und hat beschlossen, im Rahmen einer Umgestaltung des Gesch\xe4ftsmodells Geb\xfchren f\xfcr Unternehmenskunden zu erheben. Ab dem 31.01.2022 wird jegliche Nutzung von Docker Desktop f\xfcr Nutzer in Unternehmen mit mehr als 250 Besch\xe4ftigten oder einem Jahresumsatz von mehr als 10 Millionen Dollar kostenpflichtig sein. Ein Abonnement kostet dann je nach weiten genutzten Diensten und Zahlungsweise zwischen 7$ und 21$ pro Nutzer und Monat. F\xfcr Unternehmen und Beh\xf6rden stellt sich daher die Frage, ob Docker Desktop die monatliche Geb\xfchr wert ist oder ob es andere Optionen gibt.\\r\\n\\r\\n#### Docker Deamon und Docker Client weiterhin free and open source (FOSS)\\r\\nDer Docker Software Stack besteht neben der zuk\xfcnftig kostenpflichtigen GUI aus kostenfreien, quelloffenen Komponenten, die die eigentliche Arbeit machen. Der Docker-Client ist ein Befehlszeilendienstprogramm, das die API des Docker Daemon bedient. Der Docker Deamon bildet das Herzst\xfcck der Container Laufzeitumgebung. Unter Linux k\xf6nnen diese nativ und kostenfrei installiert und genutzt werden.\\r\\n\\r\\n#### Neue Wege unter Windows mit infologistix\\r\\nUnter Microsoft Windows 10 und Windows 11 muss man hingegen einen etwas aufw\xe4ndigeren Weg beschreiten, um dem Lizensierungsmodell zu entgehen und Docker weiterhin kostenfrei auf dem lokalen PC zu nutzen. Aber wir von infologistix haben die Verwaltung dieser Tools so einfach gemacht wie die Nutzung des Internets. Von gro\xdfen Rechenzentren abgeschaut, in denen Linux Server Docker als FOSS ausf\xfchren, nutzen wir ebendiese Tools f\xfcr die lokalen Entwicklungsumgebung.\\r\\n\\r\\n**Wir haben die Installation von Docker so einfach gestaltet, dass nach der Aktivierung und Installation des Windows Subsystem f\xfcr Linux nur noch ein Installationsprogramm ausgef\xfchrt werden muss. Als Bonus k\xf6nnen Sie den Docker-Client selbst auf Ihrem lokalen Rechner installieren, indem Sie ihn herunterladen und einen Docker-Kontext konfigurieren, um die intern gehostete Docker-Plattform auf Windows Subsystem f\xfcr Linux zu benutzen.**\\r\\n\\r\\n### Los geht\u2019s\\r\\n##### Installation von WSL2\\r\\nDer erste Schritt um auch unter Windows 10 oder Windows 11 Docker native auf einem Linux Kernel laufen zu lassen ist die Aktivierung des neuen Windows Subsystem for Linux (WSL2):\\r\\n\\r\\n##### Aktuelle Builds\\r\\nWenn Sie Windows 10, Version 2004 und h\xf6her (Build 19041 und h\xf6her), oder Windows 11 ausf\xfchren k\xf6nnen Sie einfach Ihre Power Shell als Administrator \xf6ffnen und folgenden Befehl ausf\xfchren:\\r\\n```\\r\\n$ wsl --install\\r\\n```\\r\\nDieser Befehl aktiviert die erforderlichen optionalen Windows Komponenten, l\xe4dt den aktuellen Linux-Kernel herunter, legt WSL 2 als Standard fest und installiert eine Linux-Distribution f\xfcr Sie (standardm\xe4\xdfig Ubuntu). Um die installierte Distribution zu \xe4ndern, geben Sie Folgendes ein:\\r\\n```\\r\\n$ wsl --install -d <Distribution Name>\\r\\n```\\r\\nErsetzen Sie < Distribution Name >  durch den Namen der Distribution, die Sie installieren m\xf6chten.\\r\\n##### \xc4ltere Builds\\r\\nWindows 10 biete ab dem Fall Creators Update (Version 1709) das Windows Subsystem for Linux (WSL) in der Version 1 an. WSL2 ist nach erweiterter Abw\xe4rtskompatibilit\xe4t ab Build 18363.1049 verf\xfcgbar. F\xfcr die folgenden Schritte beziehen wir uns immer auf die 64-Bit Variante der PowerShell.\\r\\n\\r\\nAls erstes muss WSL als Windows Feature aktiviert werden. Rufen Sie dazu die PowerShell als Administrator auf und aktivieren sie das Feature:\\r\\n```\\r\\n$ dism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all /norestart\\r\\n```\\r\\nVor der Installation von WSL 2 m\xfcssen Sie das optionale Feature Plattform des virtuellen Computers aktivieren.\\r\\n\\r\\n\xd6ffnen Sie die PowerShell als Administrator und f\xfchren Sie Folgendes aus:\\r\\n```\\r\\n$ dism.exe /online /enable-feature /featurename:VirtualMachinePlatform /all /norestart\\r\\n```\\r\\nStarten Sie anschlie\xdfend den Rechner neu.\\r\\n\\r\\nInstallieren Sie nun das WSL-Update-Paket. Hierzu laden Sie das aktuelle Update-Paket unter https://wslstorestorage.blob.core.windows.net/wslblob/wsl_update_x64.msi herunter. F\xfchren Sie das im vorherigen Schritt heruntergeladene Updatepaket aus.\\r\\n\\r\\nStarten Sie nun eine PowerShell (ohne Administratorrechte) und f\xfchren Sie Folgendes aus um WSL2 als Standardversion zu setzen\\r\\n```\\r\\n$ wsl --set-default-version 2\\r\\n```\\r\\n\xd6ffnen Sie den Microsoft Store, und w\xe4hlen Sie Ihre bevorzugte Linux-Distribution aus. Unser Skript und das hier beschriebene Vorgehen unterst\xfctzt:\\r\\n- Debian\\r\\n- Ubuntu\\r\\n- OpenSUSE\\r\\n\\r\\nWenn Sie eine neu installierte Linux-Distribution zum ersten Mal starten, wird ein Konsolenfenster ge\xf6ffnet, und Sie werden aufgefordert, zu warten, bis die Dateien dekomprimiert und auf dem Computer gespeichert wurden. Alle zuk\xfcnftigen Starts sollten weniger als eine Sekunde in Anspruch nehmen.\\r\\n\\r\\n### Infologistix Docker Installer f\xfcr WSL2\\r\\n![Aufbau der infologistix Docker L\xf6sung](img/docker-wsl-aufbau.png)\\r\\nAbbildung: Aufbau der infologistix Docker L\xf6sung\\r\\n\\r\\n##### Installation\\r\\nDocker kann mit folgendem Bash-Befehl innerhalb der WSL2 installiert werden:\\r\\n```\\r\\n$ bash <(curl -fsSL https://raw.githubusercontent.com/infologistix/docker-wsl2/main/install.sh)\\r\\n```\\r\\n##### Nutzung von Docker unter Windows\\r\\nUnter Windows ist die Pfadvariable hinzuzuf\xfcgen. Der Installer installier einen Docker Client nach **C:\\\\Docker\\\\docker.exe**. Dieser Pfad muss einmalig angegeben werden, dann ist Docker auch in Windows vorhanden.\\r\\n\\r\\nDie Benutzung erfolgt dann mit einem docker context:\\r\\n```\\r\\n$ docker context create wsldocker --docker host=tcp://localhost:2375\\r\\n$ docker context use wsldocker\\r\\n```\\r\\n##### Deinstallation\\r\\nDocker kann mit folgendem Bash-Befehl innerhalb der WSL2 deinstalliert werden:\\r\\n```\\r\\n$ bash <(curl -fsSL https://raw.githubusercontent.com/infologistix/docker-wsl2/main/uninstall.sh)\\r\\n```\\r\\n### Fazit\\r\\nZusammenfassend haben wir haben einen Installer f\xfcr die Ausf\xfchrung von Docker in einer Windows Subsystem f\xfcr Linux-Umgebung kombiniert und erstellt. Der daraus resultierende Installer, zus\xe4tzliche Konfiguration und Dokumentation finden Sie in unserem GitHub Repository:\\r\\n\\r\\n[infologistix/docker-wsl2: Simple and fast Docker Integration in WSL2 without using Docker Desktop. Suitable for large enterprises](https://github.com/infologistix/docker-wsl2)"},{"id":"webcrawler","metadata":{"permalink":"/kubernetes/blog/webcrawler","source":"@site/blog/2021-07-12-webcrawler.md","title":"Use Case f\xfcr einen WebCrawler","description":"WebCrawler sind eine einfache, effektive und kosteng\xfcnstige M\xf6glichkeit Websiten gezielt nach Informationen zu durchsuchen und Ihnen komprimiert zur Verf\xfcgung zu stellen. Die Programme sind damit ideal daf\xfcr geeignet repetitive Aufgaben zu erledigen.","date":"2021-07-12T00:00:00.000Z","formattedDate":"12. Juli 2021","tags":[{"label":"Python3","permalink":"/kubernetes/blog/tags/python-3"},{"label":"Azure","permalink":"/kubernetes/blog/tags/azure"}],"readingTime":7.49,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"webcrawler","title":"Use Case f\xfcr einen WebCrawler","tags":["Python3","Azure"]},"prevItem":{"title":"Docker in WSL","permalink":"/kubernetes/blog/docker-in-wsl"}},"content":"WebCrawler sind eine einfache, effektive und kosteng\xfcnstige M\xf6glichkeit Websiten gezielt nach Informationen zu durchsuchen und Ihnen komprimiert zur Verf\xfcgung zu stellen. Die Programme sind damit ideal daf\xfcr geeignet repetitive Aufgaben zu erledigen.\\r\\n\x3c!--truncate--\x3e\\r\\nIn diesem Artikel stellen wir Ihnen einen Use Case f\xfcr einen WebCrawler vor und geben Ihnen ein ausf\xfchrliches How-To zur Einrichtung eines cloudbasierten Dockers.\\r\\n### WebCrawler - Use Case\\r\\nWebCrawler sind leichtgewichtige und kosteneffiziente Datensammler, die Ihnen einen Informationsvorsprung sichern.\\r\\nHeutzutage sind (fast) alle Informationen online verf\xfcgbar. Googeln ist ein Synonym f\xfcr das Finden von Informationen geworden, vom einfachen Kochrezept bis hin zur wissenschaftlichen Abhandlung.\\r\\n\\r\\nDie meisten dieser Daten sind frei zug\xe4nglich und k\xf6nnen einfach abgerufen werden. Die schiere Menge an Daten und Websites macht eine Suche aber sehr zeit- und personalintensiv. Der Zeitaufwand spielt insbesondere dann eine Rolle, wenn die Suche nicht nur einmalig geschehen soll, sondern regelm\xe4\xdfig, um dann die \xc4nderungen und Entwicklungen zu monitoren. Hier bieten sich sogenannte WebCrawler als eine einfache und vor allem kosteneffiziente M\xf6glichkeit an, die Suche zu automatisieren.\\r\\n\\r\\nWebCrawler sind Programme, die automatisiert das Internet durchsuchen und Websites analysieren. Sie sind damit perfekt geeignet, um repetitive Aufgaben zu erledigen.\\r\\n\\r\\n#### Anwendungsbeispiel\\r\\nBei der infologistix GmbH verwenden wir WebCrawler f\xfcr verschiedene Aufgaben, u.a. um \xf6ffentliche Ausschreibungsseiten zu monitoren und die f\xfcr uns interessanten Ausschreibungen heraus zu filtern. Dazu crawlen wir ca. zwei Dutzend verschiedene Websites von Beh\xf6rden, Unternehmen und Portalen. Crawlen bedeutet hier, dass ein cloud-basierter <u>Docker</u> die Seiten nacheinander aufruft, sie nach interessanten Ausschreibungen durchsucht und uns eine Ergebnisliste mit ausschlie\xdflich relevanten Ausschreibungen zur Verf\xfcgung stellt. Technische Details, sowie das Dockerimage und eine Anleitung zum Nachbauen finden Sie im folgenden [How-To Abschnitt](#webcrawler---how-to).\\r\\n\\r\\nDie Ergebnisliste mit relevanten Ausschreibungen stellt f\xfcr uns eine enorme Zeit- und Kostenersparnis dar. Anstatt hunderte von Ausschreibungen pro Tag durchsuchen zu m\xfcssen, m\xfcssen jetzt nur noch ein halbes Dutzend Ausschreibung gepr\xfcft werden. Und das bei nur ca. 1\u20ac Betriebskosten pro Woche f\xfcr den WebCrawler.\\r\\n\\r\\n#### Res\xfcmee\\r\\nWebCrawler sind eine einfache, effektive und kosteng\xfcnstige M\xf6glichkeit, Websites gezielt nach Informationen zu durchsuchen und Ergebnisse komprimiert zur Verf\xfcgung zu stellen. Unser Dockerimage stellt da eine besonders leichtgewichtige und easy to-use Variante dar. Sollten Sie Fragen zu WebCrawlern haben oder eine ma\xdfgeschneiderte L\xf6sung suchen, kontaktieren Sie uns gerne.\\r\\n\\r\\n### WebCrawler - How-To\\r\\nMit diesem How-To zeigen wir, wie man einen Cloud-basierten Docker einrichtet, um damit Websites nach Informationen zu durchsuchen und aufbereitete Suchergebnisse mittels Benachrichtigung \xfcber Social Chat wie MS Teams oder Slack oder auch E-Mail zur Verf\xfcgung zu stellen.\\r\\n\\r\\n####  Was man daf\xfcr ben\xf6tigt\\r\\n- Lokale Docker Runtime\\r\\n- Social Chat (MS Teams, Slack) oder E-Mail Client\\r\\n- [infologistix WebCrawler von Docker Hub](https://hub.docker.com/r/infologistix/docker-selenium-python)\\r\\n- Etwas Erfahrung mit [Python](https://www.python.org/), Command Line, HTML\\r\\n\\r\\n#### F\xfcr die Anwendung in der Cloud wahlweise:\\r\\n- [Microsoft Azure](https://azure.microsoft.com/en-us/)\\r\\n- [Amazon AWS](https://aws.amazon.com/)\\r\\n- [Google Cloud Platform](https://cloud.google.com/)\\r\\n\\r\\n#### Legen wir los\\r\\n<u> 0. GitHub Repository Klonen </u>\\r\\n\\r\\nDieses How-To basiert auf einer Projektstruktur, welche wir bereits angelegt haben und, die Sie direkt in Ihr Projekt \xfcbernehmen k\xf6nnen.\\r\\nUnter dem folgenden Link finden Sie dieses How-To auch **als fertiges Programm im Ordner Examples.**\\r\\n```\\r\\n$ git clone https://github.com/infologistix/docker-selenium-python.git ./infologistix\\r\\n```\\r\\nWir wollen hier ein einfaches Beispiel betrachten und zun\xe4chst herausfinden, welche Services unser Unternehmen bietet. Die hier gezeigten Funktionen lassen sich einfach auf neue Gegebenheiten anpassen\\r\\n\\r\\n<u> 1. Grundstruktur und Bibliotheken </u>\\r\\n\\r\\nDas Test-Framework Selenium bietet uns bereits eine gro\xdfe Auswahl an Funktionen und Equipment, um Websites gezielt zu bearbeiten und Informationen zu ermitteln. Die Grundstruktur unseres Projektes basiert auf einer Klasse, welche die Kommunikation mit dem Browser \xfcbernimmt und uns s\xe4mtliche Daten aus der Webseite extrahiert. Dar\xfcber hinaus verwenden wir pandas als Tool f\xfcr die Formatierung und Datenanalyse der Suchergebnisse.\\r\\n```\\r\\nfrom selenium.webdriver import Chrome\\r\\nfrom selenium.webdriver import ChromeOptions \\r\\nfrom selenium.webdriver.support.ui import WebDriverWait\\r\\nfrom selenium.webdriver.common.by import By\\r\\nfrom selenium.webdriver.support import expected_conditions as EC\\r\\nimport pandas as pd\\r\\n```\\r\\nAusgehend von den genutzten Bibliotheken k\xf6nnen wir eine Basisklasse f\xfcr unseren Crawler schreiben. Diese Basisklasse beschreibt den allgemeinen Crawler. Wir verwenden die Klassendarstellung, da hier die Website und deren Funktionen und Elementextraktion als Variablen und Funktionen aufgerufen werden k\xf6nnen. Initial \xf6ffnen wir hiermit ein Chrome-Fenster im \u201eheadless\u201c-Modus und \xf6ffnen die gegebene Webseite.\\r\\n```\\r\\nclass InfologistixCrawler():\\r\\n    def __init__(self, url: str, headless: bool=True) -> None:\\r\\n        options = ChromeOptions()\\r\\n        options.add_argument(\\"\u2013no-sandbox\\")\\r\\n        options.add_argument(\\"\u2013window-size=1280,720\\")\\r\\n        if headless:\\r\\n            options.add_argument(\\"\u2013headless\\")\\r\\n        self.driver = Chrome(options=options)\\r\\n        self.driver.get(url)\\r\\n\\r\\n    def run(self):\\r\\n        page = self.driver.page_source\\r\\n        self.close()\\r\\n        return page\\r\\n\\r\\n    def close(self):\\r\\n        self.driver.close()\\r\\n```\\r\\n<u> 2. Informationen der Webseite ermitteln </u>\\r\\n\\r\\nWir filtern aus unserer eigenen [Homepage](https://infologistix.de/) die angebotenen Dienstleistungen heraus und wollen diese mit Namen, Details und Referenz abspeichern. Dazu suchen wir uns die Informationen auf der Website und ermitteln die HTML-Grundstruktur. In unserem Fall befinden sich die Dienstleistungen in einem \u201asection\u2018-Element, welches wir finden m\xfcssen. Unser gesuchtes Element besitzt die ID \u201aLeistungen\u2018, welches wiederum mehrere \u201asection\u2018-Elemente mit dem Klassenattribut \u201aelementor-image-box-content\u2018 beinhaltet. Hier sind alle Dienstleistungen hinterlegt.\\r\\n\\r\\nZun\xe4chst warten wir, bis das gesuchte Element vorhanden ist. Wir teilen mit WebDrverWait unserem Programm mit, maximal 10 Sekunden darauf zu warten, dass unser gefordertes Element auf der Webseite im DOM vorhanden ist. Ist dieses Element nicht vorhanden, dann wird hier aus dem laufenden Programm ausgestiegen und man l\xe4uft in keine Fehler.\\r\\n\\r\\nDann speichern wir uns den Container mit den gesuchten Elementen mittels der find_element_by\\\\*-Funktion und suchen innerhalb dieses Containers mit find_elements_by* alle gesuchten Elemente und extrahieren die einzelnen Informationen der Extraktion.\\r\\n```\\r\\ndef getServices(self) -> list:\\r\\n        results = list()\\r\\n        WebDriverWait(self.driver, 10).until(EC.presence_of_element_located((By.ID, \\"Leistungen\\")))\\r\\n        services = self.driver.find_element_by_id (\\"Leistungen\\")\\r\\n        for service in services.find_elements_by_tag_name(\\"section\\"):\\r\\n            results.append(\\r\\n                 self.__extract(\\r\\n                          service.find_element_by_class_name(\\r\\n                                   \\"elementor-image-box-content\\")))\\r\\n\\r\\n         return results\\r\\n```\\r\\nAls Ergebnis erhalten wir eine Liste, welche wir nur noch zur\xfcckgeben m\xfcssen. Was genau die Funktion __extract() macht, das erkl\xe4ren wir gleich\\r\\n\\r\\nMit Hilfe der Liste l\xe4sst sich jetzt eine Darstellung der einzelnen Leistungen entwickeln, welche die Sichtbarkeit und Lesbarkeit f\xfcr den Menschen vereinfacht. Wir w\xe4hlen hierbei die Darstellung eines Pandas DatenFrames, da hier z.B. auch numerische Berechnungen und Aggregationen vorgenommen werden k\xf6nnen. Zus\xe4tzlich k\xf6nnen hieraus auch Excel-Dateien erstellt werden. Wir bilden hier also einen DataFrame mit den Spalten \u201eURI\u201c, \u201eTitle\u201c und \u201eDescription\u201c aus unserer Liste, in welchem der Titel, Beschreibung und die jeweilige Referenz steht.\\r\\n\\r\\nZur\xfcckgegeben wird dann ein DatenFrame.\\r\\n```\\r\\ndef makeFrame(self, services: list) -> pd.DataFrame:\\r\\n    return pd.DataFrame(services)\\r\\n```\\r\\nAuf Basis dieser Transformation der Ergebnisse stellen wir unsere run() Funktion um, sodass sie uns die gefundenen Ergebnisse liefert und die erstellten Funktionen einbindet.\\r\\n```\\r\\ndef run(self):\\r\\n     services = self.getServices()\\r\\n     self.close()\\r\\n     return self.makeFrame(services)\\r\\n```\\r\\nErinnern wir uns zur\xfcck an unsere __extract() Funktion. Hier wollen wir aus den Elementen die erforderlichen Informationen extrahieren. Relevante Informationen herauszufinden und zu filtern, ist hier die Hauptaufgabe. S\xe4mtliche Informationen sind in unterschiedlichen Elementen mit unterschiedlichen Identifizierungsm\xf6glichkeiten vorhanden. Der Titel steht zum Beispiel als Text innerhalb eines \u201aa\u2018-Elements, welches zus\xe4tzlich noch die Referenz beinhaltet. In unserem Beispiel setzt sich die Extraktionsfunktion wie folgt zusammen und gibt uns ein geordnetes Dictionary zur\xfcck, in welchem die Elementinformationen enthalten sind.\\r\\n```\\r\\ndef __extract(self, service: WebElement) -> dict:\\r\\n        return {\\r\\n           \\"URI\\": service.find_element_by_tag_name(\\"a\\").get_attribute(\\"href\\"),\\r\\n           \\"Title\\": service.find_element_by_tag_name(\\"a\\").text,\\r\\n           \\"Description\\": service.find_element_by_tag_name(\\"p\\").text,\\r\\n         }\\r\\n```\\r\\nDie gesamte Klasse mit Funktionen sieht dann wie folgt aus:\\r\\n```\\r\\nclass InfologistixCrawler():\\r\\n    def __init__(self, url, headless=False):\\r\\n        options = ChromeOptions()\\r\\n        options.add_argument(\\"\u2013no-sandbox\\")\\r\\n        options.add_argument(\\"\u2013window-size=1280,720\\")\\r\\n        if headless:\\r\\n            options.add_argument(\\"\u2013headless\\")\\r\\n        self.driver = Chrome(options=options)\\r\\n        self.driver.get(url)\\r\\n\\r\\n    def getServices(self) -> list:\\r\\n        results = list()\\r\\n        WebDriverWait(self.driver, 10).until(EC.presence_of_element_located((By.ID, \\"Leistungen\\")))\\r\\n        services = self.driver.find_element_by_id (\\"Leistungen\\")\\r\\n        for service in services.find_elements_by_tag_name(\\"section\\"):\\r\\n            results.append(\\r\\n                 self.__extract(\\r\\n                          service.find_element_by_class_name(\\r\\n                                   \\"elementor-image-box-content\\")))\\r\\n        return results\\r\\n\\r\\n    def __extract(self, service: WebElement) -> dict:\\r\\n        return {\\r\\n           \\"URI\\": service.find_element_by_tag_name(\\"a\\").get_attribute(\\"href\\"),\\r\\n           \\"Title\\": service.find_element_by_tag_name(\\"a\\").text,\\r\\n           \\"Description\\": service.find_element_by_tag_name(\\"p\\").text,\\r\\n         }\\r\\n\\r\\n    def makeFrame(self, services : list) -> pd.DataFrame:\\r\\n        return pd.DataFrame(services)\\r\\n\\r\\n    def run(self) -> pd.DataFrame:\\r\\n        services = self.getServices()\\r\\n        self.close()\\r\\n        return self.makeFrame(services)\\r\\n\\r\\n    def close(self) -> None:\\r\\n        self.driver.close()\\r\\n```\\r\\nLassen wir unseren Crawler laufen, dann erhalten wir einen DataFrame mit den angebotenen Leistungen der infologistix GmbH.\\r\\n```\\r\\nservices = Crawler(url=\\"https://infologistix.de\\").run()\\r\\n```\\r\\nWir k\xf6nnen nun die Dienstleistungen der infologistix GmbH von der Website extrahieren und als Dataframe ausgeben lassen. Der n\xe4chste Schritt ist die \xdcbermittlung der Ergebnisse.\\r\\n<u> 3. \xdcbermittlung & Messaging </u>\\r\\nF\xfcr Teams, als auch Slack wird ein Token beziehungsweise Webhook verwendet, um eine formatierte Nachricht an einen Channel zu senden. Die Integration von Slack ist leider noch nicht vollst\xe4ndig umgesetzt und kann daher zu Fehlern f\xfchren.\\r\\n\\r\\n[Hier findest du eine detaillierte Anleitung, wie man an eine Webhook eines Kanals in MS Teams gelangt.](https://dev.outlook.com/Connectors/GetStarted#creating-messages-through-office-365-connectors-in-microsoft-teams)\\r\\n\\r\\nHier benutzen wir die Basisbibliothek pymsteams und erstellen eine Card mit der uns verf\xfcgbaren Webhook URL. Wir f\xfcgen dem Beitrag einen Titel und die als HTML formatierten Suchergebnisse an und schicken ihn an unseren Teams-Channel.\\r\\n```\\r\\nimport pymsteams\\r\\n\\r\\nmessage = services.to_html()\\r\\ntitle = \\"Dienstleistungen Infologistix\\"\\r\\n\\r\\ndef sendMSTeams(webhook : str, message : str, title : str) -> Literal[True]:\\r\\n    channel = pymsteams.connectorcard(webhook)\\r\\n    channel.title(title)\\r\\n    channel.text(message)\\r\\n    return channel.send()\\r\\n```\\r\\nWir haben nun unseren Ablauf geplant und k\xf6nnen diesen als main.py speichern.\\r\\n<u> 4. Cloud-Container </u>\\r\\nMit dem hier erstellten Script k\xf6nnen wir jetzt einen Docker-Container erstellen, welcher uns die M\xf6glichkeit gibt, den Crawler in der Cloud laufen zu lassen. Hierzu erstellen wir das Dockerfile oder nehmen das bereits vorhandene Beispiel auf.\\r\\n\\r\\nEs reicht hier ein einfacher Befehl um dieses Beispiel laufen zu lassen.\\r\\n```\\r\\n$ docker run -it \u2013rm \u2013shm-size=128m docker-selenium-python:latest python ./examples/main.py\\r\\n```\\r\\nEine Anleitung f\xfcr das Einrichten in der Cloud-Umgebung findest du hier:\\r\\n\\r\\n- [Microsoft Azure](https://learn.microsoft.com/de-de/azure/container-instances/container-instances-tutorial-prepare-app)\\r\\n- [Amazon AWS](https://us-east-1.signin.aws.amazon.com/oauth?SignatureVersion=4&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=ASIAQHJ3VXZJKWI3EBO4&X-Amz-Date=2023-05-15T15%3A08%3A10.140Z&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEO%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCIQChMDyBnLd2%2F6pRONs4Qt%2ByvBbsOpAnL9akqgT%2FbmPzHQIgQDYigveW%2FZwaS10R4VZkdtTA08gEFv%2FXedWIz5r%2ByhkqzQMIGBACGgwwMTU2OTQ0MTMzOTQiDHXEpnkLczxPEFM%2B1SqqAzRfwLNltRQTxPe6NJdMLiwSKoLDoKdz0cfX%2B3V1qQ9SLmVKLxC3Ar3a7JNJzQ2P%2Fj7nrb5FGWM2FhQ8GUt6Zr1YE%2F%2FqvVm1%2FESeKDWHlsJMg0fF8IxeY9vQSVPE59zHD4IP94QeLfiHQPY0zO8fIY98FH6SBw18a%2FJh4AbGv20QF92lXwqVVJlmeyfdKdOTP9%2FjPwAZV1F8FItFqkqkOb8r8p8QwvPEDPr%2B0CqOdOQ631qdOtT%2FBYsOcXcvqBxeAyx1ud2SsvHC1bxG7eFdUGPrigT%2B08LU6rWtabEzNFFsVHzYV45K4o56JTO8%2BeFeKgri7N5qAINjmWAKzBRpSvF2zLx8i5Y%2BLYLx278yZwaNRSTCj%2FzaDkI%2BSP1cqU2qLStDbjXD51CrA3RSVxMjUn7%2BqqF48ynxogK2gsPeNAXSpt4RUwen28SINrnzw6ajQHxU3bxKa8pHx4sIe%2Fq6%2BmsHNdwP9EJsQ5WT0Dsd16F29IVrSfOPbN7azqPuCmVT5HmXtC13WsltPEHL4LHYlGCNqWQy2yIe0ivXLdoEmTEl1aIzwUpWCi0uEjDUgYmjBjqhAYubtfW9U3AJzxe55%2FXr2Ps%2FTiqGV8tWTNhFESUbyVKKXPXeFrbvrQd%2BXNUhXekARqS4aqbyiqmNxy3HyoxWFXnQkIrZHNZfj8kORXtj6QpWwGtYi7J5kh9TOAyW1GASQpj5CLuk8LnvsBLsAqmdSmuxD69GwoaZ1%2FqcSd441OA1uIiaYjeVM4MAU7Z%2F5hKmgLFacJZwD7UaHq7BNIjt2YZ8&X-Amz-Signature=88941b4e1ab60b16a86d29aa30169819c908f3b5558268f10040ef6c273940e1&X-Amz-SignedHeaders=host&client_id=arn%3Aaws%3Aiam%3A%3A015428540659%3Auser%2Fecs&code_challenge=Rzvs-__JFcYEHVU8L8WkBiXx4NhaNcxiZN2krlbe1XQ&code_challenge_method=SHA-256&redirect_uri=https%3A%2F%2Fus-east-1.console.aws.amazon.com%2Fecs%2Fhome%3Fregion%3Dus-east-1%26state%3DhashArgs%2523%252FgetStarted%26isauthcode%3Dtrue&region=us-east-1&response_type=code&state=hashArgs%23%2FgetStarted)\\r\\n- [Google Cloud Services](https://cloud.google.com/container-registry/docs/pushing-and-pulling?hl=de)"}]}')}}]);