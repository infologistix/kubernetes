"use strict";(self.webpackChunkkubernetes=self.webpackChunkkubernetes||[]).push([[1477],{10:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"testdata","metadata":{"permalink":"/blog/testdata","source":"@site/blog/2022-10-20-testdata.md","title":"Test data sampling","description":"Before new applications go into production, data-driven tests are essential to ensure the quality of software and applications. For these tesings we have built a test data generator.","date":"2022-10-20T00:00:00.000Z","formattedDate":"October 20, 2022","tags":[{"label":"Python3","permalink":"/blog/tags/python-3"},{"label":"Azure","permalink":"/blog/tags/azure"}],"readingTime":1.52,"hasTruncateMarker":true,"authors":[{"name":"Marie Padberg","title":"Data Scientist","url":"https://github.com/MariePad","imageURL":"https://github.com/MariePad.png","key":"padberg"}],"frontMatter":{"slug":"testdata","title":"Test data sampling","authors":["padberg"],"tags":["Python3","Azure"]},"nextItem":{"title":"First Lecture of Cloud Native Computing","permalink":"/blog/lecture"}},"content":"Before new applications go into production, data-driven tests are essential to ensure the quality of software and applications. For these tesings we have built a test data generator.\\r\\n\x3c!--truncate--\x3e\\r\\n\\r\\nPersonal data is a core element of the entire business in many industries, e.g. insurance, banking and of course retail. The sensitive and highly-differentiated information stored here cannot be used for testing for data protection reasons. Instead, synthetic data is often used, the structure of which usually does not reflect the original data stock.\\r\\n\\r\\nThis means that realistic test conditions cannot be created. The result is significantly higher costs, because fixinings have to be installed during operation. In the worst case, this can even lead to acceptance problems among users and customers.\\r\\n\\r\\n**What is needed instead is test data that is realistic and representative of the overall data stock.** However, the creation of such test data means a lot of work in many cases, because dependencies have to be preserved, data types must not change, outliers need attention, personal data should not be used without pseudonomization, and so on.\\r\\n\\r\\n**Therefore, we have developed a test data generator that creates a representative sample from a large original data set and then pseudonomizes it..**\\r\\n\\r\\nTwo different sampling methods can be selected, which we have evaluated in advance using statistical methods. Furthermore, different pseudonomizations are available. Finally, a download of the test data and a short report, with a comparison of the original and test data, are provided.\\r\\n\\r\\nCurrently, we have deployed the entire system as an on-demand site using Azure Functions. This means that when demand is high, more resources are kept available for as long as necessary. When demand drops, resources are reduced again. Therefore, loading the page can sometimes take a few seconds.\\r\\n\\r\\nWe are still in the testing phase of the [test data generator](https://kitestdataengine.azurewebsites.net/file_upload)."},{"id":"lecture","metadata":{"permalink":"/blog/lecture","source":"@site/blog/2022-03-31-lecture.md","title":"First Lecture of Cloud Native Computing","description":"Inaugural Lecture at the University of Applied Sciences W\xfcrzburg-Schweinfurt","date":"2022-03-31T00:00:00.000Z","formattedDate":"March 31, 2022","tags":[{"label":"Cloud Native Computing","permalink":"/blog/tags/cloud-native-computing"}],"readingTime":0.68,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"lecture","title":"First Lecture of Cloud Native Computing","tags":["Cloud Native Computing"]},"prevItem":{"title":"Test data sampling","permalink":"/blog/testdata"},"nextItem":{"title":"Data Replication with Confluent Cloud","permalink":"/blog/data-Replication"}},"content":"Inaugural Lecture at the University of Applied Sciences W\xfcrzburg-Schweinfurt\\n\x3c!--truncate--\x3e\\n\\nJust in time for the start of the summer semester 2022, our Head of Cloud Native Computing held his inaugural lecture as a lecturer at the [University of Applied Sciences W\xfcrzburg-Schweinfurt](https://de.linkedin.com/school/thws/).  \\n\\nDr.-Ing. Harald Gerhards supports the university as a lecturer in the newly accredited master\'s program \\"Artificial Intelligence\\". The focus of the elective module will be that students get to know cloud-native characteristics and architecture patterns and practice using important tools from the field (Docker, Kubernetes, Apache Kafka, Git and others).\\n\\nAs a certified Gold Partner of SUSE, infologistix has been working on building cloud-native platforms for our customers for several years. In this way, we create the conditions for scalability, fast adaptations of applications, near-real-time data analysis, AI-supported evaluations and much more for our customers."},{"id":"data-Replication","metadata":{"permalink":"/blog/data-Replication","source":"@site/blog/2022-03-04-dataReplication.md","title":"Data Replication with Confluent Cloud","description":"Apache Kafka as a Service by Confluent","date":"2022-03-04T00:00:00.000Z","formattedDate":"March 4, 2022","tags":[{"label":"Confluent","permalink":"/blog/tags/confluent"},{"label":"Kafka","permalink":"/blog/tags/kafka"},{"label":"Azure","permalink":"/blog/tags/azure"}],"readingTime":8.84,"hasTruncateMarker":true,"authors":[{"name":"Harald P. Gerhards","title":"Senior IT Consultant | Head of Cloud Engineering","url":"https://github.com/HPG84","imageURL":"https://github.com/HPG84.png","key":"gerhards"},{"name":"Emmanouil Goulidakis","title":"IT Consultant","url":"https://github.com/Chainsaw7","imageURL":"https://github.com/Chainsaw7.png","key":"goulidakis"}],"frontMatter":{"slug":"data-Replication","title":"Data Replication with Confluent Cloud","authors":["gerhards","goulidakis"],"tags":["Confluent","Kafka","Azure"]},"prevItem":{"title":"First Lecture of Cloud Native Computing","permalink":"/blog/lecture"},"nextItem":{"title":"Docker in WSL","permalink":"/blog/docker-in-wsl"}},"content":"Apache Kafka as a Service by Confluent\\n\x3c!--truncate--\x3e\\n\\n### Introduction\\nThis article presents a proof of concept (PoC) of the group NewTechnologies@infologistix (NT) for cloud based replication (cbr) with Confluent cloud, the managed cloud service for Apache Kafka, and Microsoft Azure SQL databases, the database as a service on Microsoft Azure. The replication should be done by change data capture (CDC).\\nA combination of Confluent Cloud and managed Azure cloud services can be a really good choice for small enterprises as it provides the opportunity to develop serverless applications and therefore pay only for the data usage. That\u2019s not the case when running Apache Kafka at your own data center or using other data streaming services on cloud platforms. And if your business will grow, you still stay flexible: At any point, it is possible to change subscription and get a dedicated cluster with private networking.  For situations that require dedicated clusters or a huge amount of data is shipped, Confluent Platform is probably the cheaper choices than Confluent Cloud.\\n![High level structure of the PoC](img/data-replication/confluent-kafka-poc.png)\\n*Figure 1: High level structure of the PoC*\\n\\nThe use case presented here, involves replicating data from a Azure SQL Database source to a Azure SQL Database target by applying the MS SQL server source and sink connector services of Confluent Cloud.  It\u2019s a quit simple case, but a good way to get an impression how easy data replication and the usage of Confluent cloud can be. The whole configuration can be achieved just with the help of the Confluent and Microsoft Azure user interfaces. No coding is needed.\\n### Step by step to serverless data replication\\n\\n<u> 1. Configuring Azure </u>\\n\\nMicrosoft Azure offers a variety of managed services built for the cloud. First, let\u2019s see how we can create databases.  For our use case we are going to need a source and a sink database on separate database servers for each one of them.\\n\\n![Azure Portal dashboard](img/data-replication/azure-portal-dashboard.png)\\n*Figure 2: Azure Portal dashboard*\\n\\nThe first step is to log into the Azure portal. Then create a resource group. Here apart from the name of the group, the host region must be chosen. From the menu of the resource group, other services including the SQL database can be created and managed. \\n\\n![Create a resource group instance](img/data-replication/azure-portal-resource-group.png)\\n*Figure 3: Create a resource group instance*\\n\\nBut first an SQL Database server is required. So go back to the home directory and click-on the SQL Database server service. In the create menu, it is asked to assign the server to a valid resource group. The full name of the new server will be the given name plus the suffix \u201c.database.windows.net\u201d. Please keep that in mind for later configurations.  It is preferred to choose the same host location as the one for the resource group. In the authentication section, a username or so-called \u201cServer admin login\u201d and a password must be given. Finally, it is advised to allow the azure services and resources to access your server.\\n\\n![Create an SQL Server instance in the Resource group](img/data-replication/azure-sql-instance.svg)\\n*Figure 4: Create an SQL Server instance in the Resource group*\\n\\nNow all the infrastructure to set up the SQL database is available. It can be done at either the SQL server menu or the resource group menu. At the create menu, there are various options regarding  the storage, pricing and performance of the database connection to fit every need. It is again important to allow access to the azure services and to add the client IP address to the firewall. The rest of the networking configurations are meant mostly for advanced users. \\n\\n![Create an SQL database instance in each SQL Server](img/data-replication/azure-create-sql-database.png)\\n*Figure 5: Create an SQL database instance in each SQL Server*\\n\\nIn this PoC the source SQL server is called  \u201cnt-cbr-sqlserver\u201d whereas the sink server \u201cnt-cbr-sqlserver-dest\u201d. The corresponding source database is called CloudBasedReplication_Source and the sink database \u201cCloudBasedReplication_Target\u201d. The server username is \u201cInfologistix\u201d. This info will be used to configure the confluent cloud connectors.\\n\\n![Resource group Menu](img/data-replication/azure-resource-group-menu.png)\\n*Figure 6: Resource group Menu*\\n\\nThe database data can be accessed from the query editor inside the database menu or from an external client (e.g SQL Server Management Studio (SSMS)). Activity logs, properties and other information can also be found in the menu. To illustrate the data replication, a table \u201corderdetails\u201d with the following structure is created in the \u201cCloudBasedReplication_Source\u201d database and is filled with some data:\\n\\n```\\nCREATE TABLE orderdetails(\\n\\"ORDER_NUMBER\\" int identity (1,1) PRIMARY KEY,\\n\\"PRODUCT_CODE\\" int,\\n\\"QUANTITY_ORDERED\\" float,\\n\\"PRICE_EACH\\" decimal(10,5),\\n\\"CREATED_AT\\" datetime2 DEFAULT CURRENT_TIMESTAMP NOT NULL)\\n```\\n**Attention:** The timestamp column must always be of type datetime2. Other formats are not supported.\\n\\n<u> 2. Configuring Confluent Cloud </u>\\n\\nThe initial step in the confluent cloud environment is to set up a cluster. One can choose between three providers: AWS, Google Cloud and Azure. Obviously, for this use case we are going to choose the Azure provider. Furthermore, the host region and the subscription type must be chosen. A basic subscription is enough in this case. For new accounts, a free trial of 2 months and consumption up to 400 US Dollars is offered. \\n\\n![Create Cluster page](img/data-replication/cluster-page.png)\\n*Figure 7: Create Cluster page*\\n\\n![Confluent Cloud Cluster dashboard](img/data-replication/cluster-dashboard.png)\\n*Figure 8: Confluent Cloud Cluster dashboard*\\n\\nNext, a schema at the environment dashboard needs to be defined. The Avro schema should be chosen.\\nNow, click-on the cluster menu. The first thing is to create an API Key to authenticate the cluster. The API key username and password should be saved during its generation. Afterwards there is no way to retrieve the password. \\n\\n![Schema Registry overview](img/data-replication/schema-registry-overview.png)\\n*Figure 9: Schema Registry overview*\\n\\n![Create API-Key page](img/data-replication/api-key-page.png)\\n*Figure 10: Create API-Key page*\\n\\nNow it is time to set up the SQL server source and sink connectors. There is a huge variety of connectors, so it is easier to use the search bar. Each field in the configuration interface has a sufficient explanation, but there are some details that need to be treated with more care.   \\n\\n![Kafka Connector list](img/data-replication/kafka-connector-list.png)\\n*Figure 11: Kafka Connector list*\\n\\n<u>Source Connector</u>\\n\\nFirst we will add and configure the source connector:\\n![Add connector page](img/data-replication/kafka-add-connector.png)\\n*Figure 12:  Add connector page*\\n\\nThe following screenshot shows the source connector\u2019s configuration used in this example. Only the fields with a given value are visible here. The rest can be left empty, which means they will hold the default value:\\n```\\n       Source Connector Configuration:\\n{\\n  \\"connector.class\\": \\"MicrosoftSqlServerSource\\",\\n  \\"name\\": \\"MicrosoftSqlServerSourceConnector_0\\",\\n  \\"kafka.api.key\\": \\"****************\\",\\n  \\"kafka.api.secret\\": \\"****************************************************************\\",\\n  \\"topic.prefix\\": \\"src\\",\\n  \\"connection.host\\": \\"nt-cbr-sqlserver.database.windows.net\\",\\n  \\"connection.port\\": \\"1433\\",\\n  \\"connection.user\\": \\"Infologistix@nt-cbr-sqlserver\\",\\n  \\"db.name\\": \\"CloudBasedReplication_Source\\",\\n  \\"table.whitelist\\": [\\n    \\"orderdetails\\"\\n  ],\\n  \\"timestamp.column.name\\": [\\n    \\"CREATED_AT\\"\\n  ],\\n  \\"incrementing.column.name\\": \\"\\",\\n  \\"table.types\\": [\\n    \\"TABLE\\"\\n  ],\\n  \\"db.timezone\\": \\"Europe/Brussels\\",\\n  \\"numeric.mapping\\": \\"best_fit\\",\\n  \\"output.data.format\\": \\"AVRO\\",\\n  \\"tasks.max\\": \\"1\\"\\n}\\n```\\nThe name and the login credentials of the source database and server, which were created in Azure, must be given here. The example names are used to show you how to fill in the blanks correctly. The source connector creates automatically a topic with the prefix \u201csrc\u201d plus the initial table name \u201corderdetails\u201d, which serializes the table data with the Avro schema and acts like a Kafka producer. One should be careful with the prefix and table names, as specific characters might cause connector failure. The bad thing is that the confluent error messages\\nmight not identify the true cause of this problem. Underscores and dots are prohibited, whereas dash is allowed.\\n\\n<u>Sink connector</u>\\n\\nWhen adding the sink connector, proceed in the same way as for the source connector.\\n```\\n   Sink Connector Configuration:\\n\\n{\\n  \\"topics\\": [\\n    \\"srcorderdetails\\"\\n  ],\\n  \\"input.data.format\\": \\"AVRO\\",\\n  \\"connector.class\\": \\"MicrosoftSqlServerSink\\",\\n  \\"name\\": \\"MicrosoftSqlServerSinkConnector_0\\",\\n  \\"kafka.api.key\\": \\"****************\\",\\n  \\"kafka.api.secret\\": \\"****************************************************************\\",\\n  \\"connection.host\\": \\"nt-cbr-sqlserver-dest.database.windows.net\\",\\n  \\"connection.port\\": \\"1433\\",\\n  \\"connection.user\\": \\"Infologistix@nt-cbr-sqlserver-dest\\",\\n  \\"db.name\\": \\"CloudBasedReplication_Target\\",\\n  \\"insert.mode\\": \\"INSERT\\",\\n  \\"db.timezone\\": \\"Europe/Brussels\\",\\n  \\"pk.mode\\": \\"none\\",\\n  \\"auto.create\\": \\"true\\",\\n  \\"auto.evolve\\": \\"true\\",\\n  \\"batch.sizes\\": \\"1000\\",\\n  \\"tasks.max\\": \\"1\\"\\n}\\n```\\nThe name and the login credentials of the sink database need to be given here. Input format is AVRO. The sink connector consumes data from the topic \u201csrcorderdetails\u201d and load them into a table in the target database with the same name. Messages which cannot be loaded into the target are transferred it into a newly created Dead Letter Topic (in our case \u201cdlq-lcc-50d8z\u201d). Since the \u201cauto.create\u201d field is true, the table in the target Database is automatically created.\\nThis should be the overview of the connector and topic sections by now. If the schema does not seem to be set for a topic, it can be manually set:\\n\\n![Topics dashboard after all connections are set](img/data-replication/topics.png)\\n*Figure 13: Topics dashboard after all connections are set*\\n\\nFirst we will add and configure the source connector:\\n![Connector\u2019s dashboard  after all connections are set](img/data-replication/connectors.png)\\n*Figure 14: Connector\u2019s dashboard  after all connections are set*\\n\\nThe data flow can be seen at section \u201cStream lineage\u201d as below:\\n\\n![Data flow](img/data-replication/flow.png)\\n*Figure 15: Data flow*\\n\\nData are now present in the sink database:\\n\\nFirst we will add and configure the source connector:\\n![View newly transferred data in query editor  ](img/data-replication/transferred-data.png)\\n*Figure 16: View newly transferred data in query editor*\\n\\n<u>Troubleshooting</u>\\n\\nSometimes the connectors fail to run at first. If the error is unexpected then deleting and recreating can help. It can also happen that the error disappears after a couple of minutes by itself.\\n\\n<u>A few remarks about cdc connectors</u>\\n\\nNow, every time new data arrives at the source database, they will be directly processed and stored at the sink database. It should be noted though, that this is true only for rows with a timestamp value greater than the one of the last fetch. If more than one timestamp columns is defined in the source connector configuration, then the first non-null value will be considered. Same applies to modified rows. They are simply dealt as new rows, rather than updated.\\n\\n**Info:** The SQL server connectors are constantly upgraded to offer more options to the user. \\n\\n### Conclusion\\nOur proof of concept was able to show that it is extremely easy to use the functionality of Apache Kafka with the Confluent Cloud. A data replication for a table could be realized in a few minutes. However, the Confluent Cloud also offers the possibility to perform transformations through the integration of KSQL, so that ETL operations are also possible. In addition, this also enables problem-free changes of the SQL dialect, since date formats can be adapted through simple transformations. Scenarios in which an on premise database (e.g. IBM DB2 or Oracle DB) is replicated via confluent cloud and is then backed up in the Azure Cloud are therefore also no problem.\\nWe hope we have been able to give you a small insight into the possibilities of managed cloud services from our partners Confluent and Microsoft and make it easier for you to get started with cloud computing."},{"id":"docker-in-wsl","metadata":{"permalink":"/blog/docker-in-wsl","source":"@site/blog/2022-01-27-docker-in-wsl.md","title":"Docker in WSL","description":"Docker Desktop becomes commercial, we show you a free alternative - Docker in Windows Subsystem for Linux (WSL)","date":"2022-01-27T00:00:00.000Z","formattedDate":"January 27, 2022","tags":[{"label":"Docker","permalink":"/blog/tags/docker"},{"label":"WSL","permalink":"/blog/tags/wsl"}],"readingTime":4.525,"hasTruncateMarker":true,"authors":[{"name":"Suphanat Avipan","title":"IT Consultant","url":"https://github.com/suphanataviphan","imageURL":"https://github.com/suphanataviphan.png","key":"aviphan"},{"name":"Harald P. Gerhards","title":"Senior IT Consultant | Head of Cloud Engineering","url":"https://github.com/HPG84","imageURL":"https://github.com/HPG84.png","key":"gerhards"},{"name":"Nico Graap","title":"IT Consultant","url":"https://github.com/Nico-infologistix","imageURL":"https://github.com/Nico-infologistix.png","key":"graap"},{"name":"Paul Schmidt","title":"Data Scientist","url":"https://github.com/pickmylight","imageURL":"https://github.com/pickmylight.png","key":"schmidt"}],"frontMatter":{"slug":"docker-in-wsl","title":"Docker in WSL","authors":["aviphan","gerhards","graap","schmidt"],"tags":["Docker","WSL"]},"prevItem":{"title":"Data Replication with Confluent Cloud","permalink":"/blog/data-Replication"},"nextItem":{"title":"Use Case for a WebCrawler","permalink":"/blog/webcrawler"}},"content":"Docker Desktop becomes commercial, we show you a free alternative - Docker in Windows Subsystem for Linux (WSL)\\r\\n\x3c!--truncate--\x3e\\r\\n\\r\\nDocker\'s free products are used by millions of developers to build, publish and run applications - in data centers, the public cloud or with Docker Desktop on the local PC. \\r\\n55% of developers use Docker every day at work.\\r\\n\\r\\n#### Docker Desktop becomes commercially viable\\r\\nBut the Docker Company also needs to be profitable and has decided to charge enterprise customers as part of a business model redesign. Starting Jan. 31, 2022, any use of Docker Desktop will be chargeable for users at companies with more than 250 employees or more than $10 million in annual revenue. A subscription will then cost between $7 and $21 per user per month, depending on which services are widely used and how they are paid for. For enterprises and government agencies, this raises the question of whether Docker Desktop is worth the monthly fee or if there are other options.\\r\\n\\r\\n#### Docker Deamon and Docker Client remain free and open source (FOSS)\\r\\nIn addition to the GUI, which will be paid for in the future, the Docker software stack consists of free, open source components that do the actual work. The Docker Client is a command-line tool that serves the Docker Daemon API. The Docker Deamon is at the heart of the container runtime environment. On Linux, these can be installed and used natively and free of charge.\\r\\n\\r\\n#### New ways under Windows with infologistix\\r\\nOn Microsoft Windows 10 and Windows 11, on the other hand, you have to take a slightly more elaborate route to escape the licensing model and continue using Docker for free on your local PC. But at infologistix, we\'ve made managing these tools as easy as using the Internet. Copying from large data centers where Linux servers run Docker as FOSS, we use these same tools for the local development environment.\\r\\n\\r\\n**We\'ve made installing Docker as simple as running an installer after enabling and installing the Windows Subsystem for Linux. As a bonus, you can install the Docker client yourself on your local machine by downloading it and configuring a Docker context to use the internally hosted Docker platform on Windows Subsystem for Linux.**\\r\\n\\r\\n### Let\'s go\\r\\n##### Installing WSL2\\r\\nThe first step to running Docker native on a Linux kernel even on Windows 10 or Windows 11 is to enable the new Windows Subsystem for Linux (WSL2):\\r\\n\\r\\n##### Current Builds\\r\\nIf you are running Windows 10, version 2004 and above (build 19041 and above), or Windows 11 you can simply open your power shell as administrator and run the following command:\\r\\n```\\r\\n$ wsl --install\\r\\n```\\r\\nThis command enables the required optional Windows components, downloads the latest Linux kernel, sets WSL 2 as the default, and installs a Linux distribution for you (Ubuntu by default). To change the installed distribution, type the following:\\r\\n```\\r\\n$ wsl --install -d <Distribution Name>\\r\\n```\\r\\nReplace < Distribution Name >  with the name of the distribution you want to install.\\r\\n##### Older builds\\r\\nWindows 10 offered Windows Subsystem for Linux (WSL) version 1 starting with the Fall Creators Update (version 1709). WSL2 is available after extended backward compatibility starting with build 18363.1049. For the following steps, we always refer to the 64-bit variant of PowerShell.\\r\\n\\r\\nThe first thing to do is to enable WSL as a Windows feature. To do this, call PowerShell as administrator and activate the feature:\\r\\n```\\r\\n$ dism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all /norestart\\r\\n```\\r\\nBefore installing WSL 2, you must enable the optional Virtual Computer Platform feature.\\r\\n\\r\\nOpen PowerShell as an administrator and run the following:\\r\\n```\\r\\n$ dism.exe /online /enable-feature /featurename:VirtualMachinePlatform /all /norestart\\r\\n```\\r\\nThen restart the computer.\\r\\n\\r\\nNow install the WSL update package. To do this, download the latest update package from  https://wslstorestorage.blob.core.windows.net/wslblob/wsl_update_x64.msi. Run the update package downloaded in the previous step.\\r\\n\\r\\nNow start a PowerShell (without administrator rights) and execute the following to set WSL2 as default version.\\r\\n```\\r\\n$ wsl --set-default-version 2\\r\\n```\\r\\nOpen the Microsoft Store and select your preferred Linux distribution. Our script and the procedure described here are supported:\\r\\n- Debian\\r\\n- Ubuntu\\r\\n- OpenSUSE\\r\\n\\r\\nWhen you start a newly installed Linux distribution for the first time, a console window will open and you will be prompted to wait until the files have been decompressed and saved to the computer. All future launches should take less than a second.\\r\\n\\r\\n### Infologistix Docker Installer f\xfcr WSL2\\r\\n![Structure of the infologistix Docker solution](img/docker-wsl-aufbau.png)\\r\\n*Figure 1: Structure of the infologistix Docker solution*\\r\\n\\r\\n##### Installation\\r\\nDocker can be installed within WSL2 using the following bash command:\\r\\n```\\r\\n$ bash <(curl -fsSL https://raw.githubusercontent.com/infologistix/docker-wsl2/main/install.sh)\\r\\n```\\r\\n##### Using Docker on Windows\\r\\nOn Windows, add the path variable. The installer will install a Docker client to **C:\\\\Docker\\\\docker.exe**. This path must be specified once, then Docker is also present in Windows. \\r\\n\\r\\nThe usage is then done with a docker context:\\r\\n```\\r\\n$ docker context create wsldocker --docker host=tcp://localhost:2375\\r\\n$ docker context use wsldocker\\r\\n```\\r\\n##### Uninstall\\r\\nDocker can be uninstalled with the following bash command inside WSL2:\\r\\n```\\r\\n$ bash <(curl -fsSL https://raw.githubusercontent.com/infologistix/docker-wsl2/main/uninstall.sh)\\r\\n```\\r\\n### Conclusion\\r\\nIn summary, we have combined and created an installer for running Docker in a Windows Subsystem for Linux environment. The resulting installer, additional configuration, and documentation can be found in our GitHub repository: \\r\\n\\r\\n[infologistix/docker-wsl2: Simple and fast Docker Integration in WSL2 without using Docker Desktop. Suitable for large enterprises](https://github.com/infologistix/docker-wsl2)"},{"id":"webcrawler","metadata":{"permalink":"/blog/webcrawler","source":"@site/blog/2021-07-12-webcrawler.md","title":"Use Case for a WebCrawler","description":"WebCrawlers are a simple, effective and inexpensive way to search websites for specific information and make it available in compressed form. The programs are thus ideally suited to perform repetitive tasks.","date":"2021-07-12T00:00:00.000Z","formattedDate":"July 12, 2021","tags":[{"label":"Python3","permalink":"/blog/tags/python-3"},{"label":"Azure","permalink":"/blog/tags/azure"}],"readingTime":7.85,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"webcrawler","title":"Use Case for a WebCrawler","tags":["Python3","Azure"]},"prevItem":{"title":"Docker in WSL","permalink":"/blog/docker-in-wsl"}},"content":"WebCrawlers are a simple, effective and inexpensive way to search websites for specific information and make it available in compressed form. The programs are thus ideally suited to perform repetitive tasks.\\r\\nIn this article, we present a use case for a WebCrawler and give you a detailed how-to on setting up a cloud-based Docker.\\r\\n\x3c!--truncate--\x3e\\r\\n### WebCrawler - Use Case\\r\\nWebCrawlers are lightweight and cost-effective data collectors that give you an information edge. Nowadays, (almost) all information is available online. Googling has become synonymous with finding information, from a simple cooking recipe to a scientific paper.\\r\\n\\r\\nMost of this data is freely available and can be easily accessed. However, the sheer volume of data and websites makes a search very time-consuming and labor-intensive. The time required is particularly important if the search is not to be carried out just once, but on a regular basis, so that changes and developments can be monitored. Here, so-called WebCrawlers offer themselves as a simple and, above all, cost-efficient way to automate the search.\\r\\n\\r\\nWebCrawlers are programs that automatically search the Internet and analyze websites. They are therefore perfectly suited for repetitive tasks.\\r\\n\\r\\n#### Example\\r\\nAt infologistix GmbH, we use WebCrawler for various tasks, including monitoring public tender sites and filtering out the tenders that are of interest to us. For this purpose we crawl about two dozen different websites of public authorities, companies and portals. Here, crawling means that a cloud-based <u>Docker</u> calls up the pages one after the other, searches them for interesting tenders and provides us with a result list containing only relevant tenders. Technical details, as well as the Docker image and instructions for rebuilding it, can be found in the [How-To section](#webcrawler---how-to) below.\\r\\n\\r\\nThe results list of relevant RFPs represents a huge time and cost savings for us. Instead of having to search through hundreds of RFPs per day, we now only have to review half a dozen RFPs. And this at only about 1\u20ac operating costs per week for the WebCrawler.\\r\\n\\r\\n#### Resum\xe9\\r\\nWebCrawlers are a simple, effective and inexpensive way to search websites for specific information and to make the results available in compressed form. Our Docker image is a particularly lightweight and easy-to-use variant. If you have any questions about WebCrawlers or are looking for a customized solution, please feel free to contact us.\\r\\n\\r\\n### WebCrawler - How-To\\r\\nWith this How-To we show how to set up a cloud-based Docker to crawl websites for information and provide prepared search results via notification over social chat like MS Teams or Slack or even email.\\r\\n\\r\\n####  What you need\\r\\n- Local Docker runtime\\r\\n- Social chat (MS Teams, Slack) or email client\\r\\n- [infologistix WebCrawler from Docker Hub](https://hub.docker.com/r/infologistix/docker-selenium-python)\\r\\n- Some experience with [Python](https://www.python.org/), Command Line, HTML\\r\\n\\r\\n#### For using in the cloud optionally:\\r\\n- [Microsoft Azure](https://azure.microsoft.com/en-us/)\\r\\n- [Amazon AWS](https://aws.amazon.com/)\\r\\n- [Google Cloud Platform](https://cloud.google.com/)\\r\\n\\r\\n#### Let\'s get started\\r\\n<u> 0. Clone GitHub repository </u>\\r\\n\\r\\nThis How-To is based on a project structure, which we have already created and which you can transfer directly into your project. Under the following link you can also find this how-to as a **finished program in the Examples folder.**\\r\\n```\\r\\n$ git clone https://github.com/infologistix/docker-selenium-python.git ./infologistix\\r\\n```\\r\\nLet\'s look at a simple example here and first find out what services our company offers. The functions shown here can be easily adapted to new circumstances.\\r\\n\\r\\n<u> 1. Basic structure and libraries </u>\\r\\n\\r\\nThe test framework Selenium already provides us with a wide range of functions and equipment to target websites and to determine information. The basic structure of our project is based on a class that handles the communication with the browser and extracts all data from the website for us. In addition, we use pandas as a tool for formatting and data analysis of the search results.\\r\\n```\\r\\nfrom selenium.webdriver import Chrome\\r\\nfrom selenium.webdriver import ChromeOptions \\r\\nfrom selenium.webdriver.support.ui import WebDriverWait\\r\\nfrom selenium.webdriver.common.by import By\\r\\nfrom selenium.webdriver.support import expected_conditions as EC\\r\\nimport pandas as pd\\r\\n```\\r\\nBased on the libraries used, we can write a base class for our crawler. This base class describes the general crawler. We use the class representation, because here the website and its functions and element extraction can be called as variables and functions. Initially, we hereby open a Chrome window in \\"headless\\" mode and open the given web page.\\r\\n```\\r\\nclass InfologistixCrawler():\\r\\n    def __init__(self, url: str, headless: bool=True) -> None:\\r\\n        options = ChromeOptions()\\r\\n        options.add_argument(\\"\u2013no-sandbox\\")\\r\\n        options.add_argument(\\"\u2013window-size=1280,720\\")\\r\\n        if headless:\\r\\n            options.add_argument(\\"\u2013headless\\")\\r\\n        self.driver = Chrome(options=options)\\r\\n        self.driver.get(url)\\r\\n\\r\\n    def run(self):\\r\\n        page = self.driver.page_source\\r\\n        self.close()\\r\\n        return page\\r\\n\\r\\n    def close(self):\\r\\n        self.driver.close()\\r\\n```\\r\\n<u> 2. Determine information of the website </u>\\r\\n\\r\\nWe filter out the services offered from our own [homepage](https://infologistix.de/) and want to store them with name, details and reference. To do this, we look for the information on the website and determine the basic HTML structure. In our case, the services are in a \'section\' element, which we need to find. Our searched element has the ID \'services\', which in turn contains several \'section\' elements with the class attribute \'elementor-image-box-content\'. All the services are stored here.\\r\\n\\r\\nFirst, we wait until the element we are looking for exists. We use WebDrverWait to tell our program to wait a maximum of 10 seconds for our requested element to be present on the web page in the DOM. If this element is not present, then we get out of the running program here and we don\'t run into any errors.\\r\\n\\r\\nThen we save the container with the searched elements using the find_element_by\\\\* function and search within this container with find_elements_by* all searched elements and extract the single information of the extraction.\\r\\n```\\r\\ndef getServices(self) -> list:\\r\\n        results = list()\\r\\n        WebDriverWait(self.driver, 10).until(EC.presence_of_element_located((By.ID, \\"Leistungen\\")))\\r\\n        services = self.driver.find_element_by_id (\\"Leistungen\\")\\r\\n        for service in services.find_elements_by_tag_name(\\"section\\"):\\r\\n            results.append(\\r\\n                 self.__extract(\\r\\n                          service.find_element_by_class_name(\\r\\n                                   \\"elementor-image-box-content\\")))\\r\\n\\r\\n         return results\\r\\n```\\r\\nAs a result we get a list, which we only have to return. What exactly the function __extract() does, we will explain in a moment.\\r\\n\\r\\nWith the help of the list we can now develop a representation of the individual services, which simplifies the visibility and readability for humans. We choose here the representation of a Pandas DatenFrames, because here, for example, numerical calculations and aggregations can be made. In addition, Excel files can also be created from this. So we create a DataFrame with the columns \\"URI\\", \\"Title\\" and \\"Description\\" from our list, which contains the title, description and the respective reference.\\r\\n\\r\\nA DataFrame is then returned.\\r\\n```\\r\\ndef makeFrame(self, services: list) -> pd.DataFrame:\\r\\n    return pd.DataFrame(services)\\r\\n```\\r\\nBased on this transformation of the results, we rearrange our run() function so that it gives us the results it finds and includes the functions it creates.\\r\\n```\\r\\ndef run(self):\\r\\n     services = self.getServices()\\r\\n     self.close()\\r\\n     return self.makeFrame(services)\\r\\n```\\r\\nLet\'s remember back to our __extract() function. Here we want to extract the required information from the elements. Finding out and filtering relevant information is the main task here. All the information is present in different elements with different identifiers. For example, the title is text within an \'a\' element, which also contains the reference. In our example, the extraction function is composed as follows and returns us an ordered dictionary containing the element information.\\r\\n```\\r\\ndef __extract(self, service: WebElement) -> dict:\\r\\n        return {\\r\\n           \\"URI\\": service.find_element_by_tag_name(\\"a\\").get_attribute(\\"href\\"),\\r\\n           \\"Title\\": service.find_element_by_tag_name(\\"a\\").text,\\r\\n           \\"Description\\": service.find_element_by_tag_name(\\"p\\").text,\\r\\n         }\\r\\n```\\r\\nThe entire class with functions then looks like this:\\r\\n```\\r\\nclass InfologistixCrawler():\\r\\n    def __init__(self, url, headless=False):\\r\\n        options = ChromeOptions()\\r\\n        options.add_argument(\\"\u2013no-sandbox\\")\\r\\n        options.add_argument(\\"\u2013window-size=1280,720\\")\\r\\n        if headless:\\r\\n            options.add_argument(\\"\u2013headless\\")\\r\\n        self.driver = Chrome(options=options)\\r\\n        self.driver.get(url)\\r\\n\\r\\n    def getServices(self) -> list:\\r\\n        results = list()\\r\\n        WebDriverWait(self.driver, 10).until(EC.presence_of_element_located((By.ID, \\"Leistungen\\")))\\r\\n        services = self.driver.find_element_by_id (\\"Leistungen\\")\\r\\n        for service in services.find_elements_by_tag_name(\\"section\\"):\\r\\n            results.append(\\r\\n                 self.__extract(\\r\\n                          service.find_element_by_class_name(\\r\\n                                   \\"elementor-image-box-content\\")))\\r\\n        return results\\r\\n\\r\\n    def __extract(self, service: WebElement) -> dict:\\r\\n        return {\\r\\n           \\"URI\\": service.find_element_by_tag_name(\\"a\\").get_attribute(\\"href\\"),\\r\\n           \\"Title\\": service.find_element_by_tag_name(\\"a\\").text,\\r\\n           \\"Description\\": service.find_element_by_tag_name(\\"p\\").text,\\r\\n         }\\r\\n\\r\\n    def makeFrame(self, services : list) -> pd.DataFrame:\\r\\n        return pd.DataFrame(services)\\r\\n\\r\\n    def run(self) -> pd.DataFrame:\\r\\n        services = self.getServices()\\r\\n        self.close()\\r\\n        return self.makeFrame(services)\\r\\n\\r\\n    def close(self) -> None:\\r\\n        self.driver.close()\\r\\n```\\r\\nIf we let our crawler run, we get a DataFrame with the offered services of infologistix GmbH.\\r\\n```\\r\\nservices = Crawler(url=\\"https://infologistix.de\\").run()\\r\\n```\\r\\nWe can now extract the infologistix GmbH services from the website and output them as a dataframe. The next step is the transmission of the results.\\r\\n<u> 3. Submission & Messaging </u>\\r\\nFor Teams, as well as Slack, a token and webhook respectively is used to send a formatted message to a channel. Unfortunately, the integration of Slack is not yet fully implemented and can therefore lead to errors.\\r\\n\\r\\n[Here you can find detailed instructions on how to get a webhook of a channel in MS Teams.](https://dev.outlook.com/Connectors/GetStarted#creating-messages-through-office-365-connectors-in-microsoft-teams)\\r\\n\\r\\nHere we use the base library pymsteams and create a card with the webhook URL available to us. We add a title and the search results formatted as HTML to the post and send it to our Teams channel.\\r\\n```\\r\\nimport pymsteams\\r\\n\\r\\nmessage = services.to_html()\\r\\ntitle = \\"Dienstleistungen Infologistix\\"\\r\\n\\r\\ndef sendMSTeams(webhook : str, message : str, title : str) -> Literal[True]:\\r\\n    channel = pymsteams.connectorcard(webhook)\\r\\n    channel.title(title)\\r\\n    channel.text(message)\\r\\n    return channel.send()\\r\\n```\\r\\nWe have now planned our flow and can save it as main.py.\\r\\n<u> 4. Cloud-Container </u>\\r\\nWith the script created here, we can now create a Docker container, which will give us the ability to run the crawler in the cloud. For this, we create the Dockerfile or take the already existing example.\\r\\n\\r\\nA simple command is enough here to run this example.\\r\\n```\\r\\n$ docker run -it \u2013rm \u2013shm-size=128m docker-selenium-python:latest python ./examples/main.py\\r\\n```\\r\\nYou can find instructions for setting up in the cloud environment here:\\r\\n\\r\\n- [Microsoft Azure](https://learn.microsoft.com/de-de/azure/container-instances/container-instances-tutorial-prepare-app)\\r\\n- [Amazon AWS](https://us-east-1.signin.aws.amazon.com/oauth?SignatureVersion=4&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=ASIAQHJ3VXZJKWI3EBO4&X-Amz-Date=2023-05-15T15%3A08%3A10.140Z&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEO%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCIQChMDyBnLd2%2F6pRONs4Qt%2ByvBbsOpAnL9akqgT%2FbmPzHQIgQDYigveW%2FZwaS10R4VZkdtTA08gEFv%2FXedWIz5r%2ByhkqzQMIGBACGgwwMTU2OTQ0MTMzOTQiDHXEpnkLczxPEFM%2B1SqqAzRfwLNltRQTxPe6NJdMLiwSKoLDoKdz0cfX%2B3V1qQ9SLmVKLxC3Ar3a7JNJzQ2P%2Fj7nrb5FGWM2FhQ8GUt6Zr1YE%2F%2FqvVm1%2FESeKDWHlsJMg0fF8IxeY9vQSVPE59zHD4IP94QeLfiHQPY0zO8fIY98FH6SBw18a%2FJh4AbGv20QF92lXwqVVJlmeyfdKdOTP9%2FjPwAZV1F8FItFqkqkOb8r8p8QwvPEDPr%2B0CqOdOQ631qdOtT%2FBYsOcXcvqBxeAyx1ud2SsvHC1bxG7eFdUGPrigT%2B08LU6rWtabEzNFFsVHzYV45K4o56JTO8%2BeFeKgri7N5qAINjmWAKzBRpSvF2zLx8i5Y%2BLYLx278yZwaNRSTCj%2FzaDkI%2BSP1cqU2qLStDbjXD51CrA3RSVxMjUn7%2BqqF48ynxogK2gsPeNAXSpt4RUwen28SINrnzw6ajQHxU3bxKa8pHx4sIe%2Fq6%2BmsHNdwP9EJsQ5WT0Dsd16F29IVrSfOPbN7azqPuCmVT5HmXtC13WsltPEHL4LHYlGCNqWQy2yIe0ivXLdoEmTEl1aIzwUpWCi0uEjDUgYmjBjqhAYubtfW9U3AJzxe55%2FXr2Ps%2FTiqGV8tWTNhFESUbyVKKXPXeFrbvrQd%2BXNUhXekARqS4aqbyiqmNxy3HyoxWFXnQkIrZHNZfj8kORXtj6QpWwGtYi7J5kh9TOAyW1GASQpj5CLuk8LnvsBLsAqmdSmuxD69GwoaZ1%2FqcSd441OA1uIiaYjeVM4MAU7Z%2F5hKmgLFacJZwD7UaHq7BNIjt2YZ8&X-Amz-Signature=88941b4e1ab60b16a86d29aa30169819c908f3b5558268f10040ef6c273940e1&X-Amz-SignedHeaders=host&client_id=arn%3Aaws%3Aiam%3A%3A015428540659%3Auser%2Fecs&code_challenge=Rzvs-__JFcYEHVU8L8WkBiXx4NhaNcxiZN2krlbe1XQ&code_challenge_method=SHA-256&redirect_uri=https%3A%2F%2Fus-east-1.console.aws.amazon.com%2Fecs%2Fhome%3Fregion%3Dus-east-1%26state%3DhashArgs%2523%252FgetStarted%26isauthcode%3Dtrue&region=us-east-1&response_type=code&state=hashArgs%23%2FgetStarted)\\r\\n- [Google Cloud Services](https://cloud.google.com/container-registry/docs/pushing-and-pulling?hl=de)"}]}')}}]);